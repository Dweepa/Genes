{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt1 \n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate, Conv1D,Conv2D, Flatten, Reshape, Embedding, GRU, SpatialDropout1D, LSTM, Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from itertools import permutations\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from scipy.stats import trim_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>inchi_key</th>\n",
       "      <th>atc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NC12CC3CC(CC(C3)C1)C2</td>\n",
       "      <td>amantadine</td>\n",
       "      <td>BRD-K70330367</td>\n",
       "      <td>DKNWSYNQZKUICI-UHFFFAOYSA-N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>CS(=O)(=O)OCCCCOS(C)(=O)=O</td>\n",
       "      <td>busulfan</td>\n",
       "      <td>BRD-K23204545</td>\n",
       "      <td>COVZYZSDYWQREU-UHFFFAOYSA-N</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>CCCC(C)(COC(N)=O)COC(=O)NC(C)C</td>\n",
       "      <td>carisoprodol</td>\n",
       "      <td>BRD-A99939097</td>\n",
       "      <td>OFZCIYFFPZCNJE-UHFFFAOYSA-N</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>CCN(CC)C(=O)N1CCN(C)CC1</td>\n",
       "      <td>diethylcarbamazine</td>\n",
       "      <td>BRD-K45542189</td>\n",
       "      <td>RCKMWOKWVGPNJF-UHFFFAOYSA-N</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>CCN(CC)C(=S)SSC(=S)N(CC)CC</td>\n",
       "      <td>disulfiram</td>\n",
       "      <td>BRD-K32744045</td>\n",
       "      <td>AUZONCFQVSMFAP-UHFFFAOYSA-N</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             smiles                name             id  \\\n",
       "75            NC12CC3CC(CC(C3)C1)C2          amantadine  BRD-K70330367   \n",
       "291      CS(=O)(=O)OCCCCOS(C)(=O)=O            busulfan  BRD-K23204545   \n",
       "322  CCCC(C)(COC(N)=O)COC(=O)NC(C)C        carisoprodol  BRD-A99939097   \n",
       "491         CCN(CC)C(=O)N1CCN(C)CC1  diethylcarbamazine  BRD-K45542189   \n",
       "510      CCN(CC)C(=S)SSC(=S)N(CC)CC          disulfiram  BRD-K32744045   \n",
       "\n",
       "                       inchi_key atc  \n",
       "75   DKNWSYNQZKUICI-UHFFFAOYSA-N   N  \n",
       "291  COVZYZSDYWQREU-UHFFFAOYSA-N   L  \n",
       "322  OFZCIYFFPZCNJE-UHFFFAOYSA-N   M  \n",
       "491  RCKMWOKWVGPNJF-UHFFFAOYSA-N   P  \n",
       "510  AUZONCFQVSMFAP-UHFFFAOYSA-N   P  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = pd.read_csv(\"../data/drug_class_identification/all3.csv\")\n",
    "full = full.dropna()\n",
    "full['atc'] = full['atc'].apply(lambda x : x[0])\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our own plot function\n",
    "def scatter(x, y, subtitle=None):\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(y)\n",
    "\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 14))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(14):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = trim_mean(x[labels == i, :], axis=0, proportiontocut=0.2)\n",
    "        letter = le.inverse_transform([i])[0]\n",
    "        txt = ax.text(xtext, ytext, str(letter), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.savefig(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn = pd.read_csv('EMB_snn_4_25_16_full-75')\n",
    "snn['id'] = snn['pert_id']\n",
    "result_snn = pd.merge(snn, full, how=\"inner\").drop(['id','smiles','name','inchi_key','pert_id','Unnamed: 0'],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>e10</th>\n",
       "      <th>e11</th>\n",
       "      <th>e12</th>\n",
       "      <th>e13</th>\n",
       "      <th>e14</th>\n",
       "      <th>e15</th>\n",
       "      <th>e16</th>\n",
       "      <th>atc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350203</td>\n",
       "      <td>0.272261</td>\n",
       "      <td>0.322720</td>\n",
       "      <td>-0.415728</td>\n",
       "      <td>-0.118934</td>\n",
       "      <td>-0.274651</td>\n",
       "      <td>-0.195480</td>\n",
       "      <td>0.411698</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>-0.112954</td>\n",
       "      <td>0.226189</td>\n",
       "      <td>-0.052496</td>\n",
       "      <td>0.047790</td>\n",
       "      <td>0.034436</td>\n",
       "      <td>-0.138315</td>\n",
       "      <td>0.373745</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350203</td>\n",
       "      <td>0.272261</td>\n",
       "      <td>0.322720</td>\n",
       "      <td>-0.415728</td>\n",
       "      <td>-0.118934</td>\n",
       "      <td>-0.274651</td>\n",
       "      <td>-0.195480</td>\n",
       "      <td>0.411698</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>-0.112954</td>\n",
       "      <td>0.226189</td>\n",
       "      <td>-0.052496</td>\n",
       "      <td>0.047790</td>\n",
       "      <td>0.034436</td>\n",
       "      <td>-0.138315</td>\n",
       "      <td>0.373745</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008870</td>\n",
       "      <td>0.203011</td>\n",
       "      <td>0.045596</td>\n",
       "      <td>-0.303917</td>\n",
       "      <td>-0.486009</td>\n",
       "      <td>0.060476</td>\n",
       "      <td>0.099339</td>\n",
       "      <td>0.431585</td>\n",
       "      <td>-0.201203</td>\n",
       "      <td>-0.458416</td>\n",
       "      <td>-0.011151</td>\n",
       "      <td>-0.193102</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>-0.025093</td>\n",
       "      <td>-0.182268</td>\n",
       "      <td>0.326008</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008870</td>\n",
       "      <td>0.203011</td>\n",
       "      <td>0.045596</td>\n",
       "      <td>-0.303917</td>\n",
       "      <td>-0.486009</td>\n",
       "      <td>0.060476</td>\n",
       "      <td>0.099339</td>\n",
       "      <td>0.431585</td>\n",
       "      <td>-0.201203</td>\n",
       "      <td>-0.458416</td>\n",
       "      <td>-0.011151</td>\n",
       "      <td>-0.193102</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>-0.025093</td>\n",
       "      <td>-0.182268</td>\n",
       "      <td>0.326008</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.054629</td>\n",
       "      <td>0.270735</td>\n",
       "      <td>0.245764</td>\n",
       "      <td>-0.271058</td>\n",
       "      <td>-0.155077</td>\n",
       "      <td>0.056891</td>\n",
       "      <td>-0.155674</td>\n",
       "      <td>0.369895</td>\n",
       "      <td>0.163061</td>\n",
       "      <td>-0.047226</td>\n",
       "      <td>0.473894</td>\n",
       "      <td>-0.323181</td>\n",
       "      <td>0.010434</td>\n",
       "      <td>-0.224793</td>\n",
       "      <td>-0.183408</td>\n",
       "      <td>0.399229</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         e1        e2        e3        e4        e5        e6        e7  \\\n",
       "0  0.350203  0.272261  0.322720 -0.415728 -0.118934 -0.274651 -0.195480   \n",
       "1  0.350203  0.272261  0.322720 -0.415728 -0.118934 -0.274651 -0.195480   \n",
       "2  0.008870  0.203011  0.045596 -0.303917 -0.486009  0.060476  0.099339   \n",
       "3  0.008870  0.203011  0.045596 -0.303917 -0.486009  0.060476  0.099339   \n",
       "4  0.054629  0.270735  0.245764 -0.271058 -0.155077  0.056891 -0.155674   \n",
       "\n",
       "         e8        e9       e10       e11       e12       e13       e14  \\\n",
       "0  0.411698  0.002516 -0.112954  0.226189 -0.052496  0.047790  0.034436   \n",
       "1  0.411698  0.002516 -0.112954  0.226189 -0.052496  0.047790  0.034436   \n",
       "2  0.431585 -0.201203 -0.458416 -0.011151 -0.193102  0.009726 -0.025093   \n",
       "3  0.431585 -0.201203 -0.458416 -0.011151 -0.193102  0.009726 -0.025093   \n",
       "4  0.369895  0.163061 -0.047226  0.473894 -0.323181  0.010434 -0.224793   \n",
       "\n",
       "        e15       e16 atc  \n",
       "0 -0.138315  0.373745   D  \n",
       "1 -0.138315  0.373745   G  \n",
       "2 -0.182268  0.326008   D  \n",
       "3 -0.182268  0.326008   G  \n",
       "4 -0.183408  0.399229   D  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_snn = result_snn['atc']\n",
    "X_snn = result_snn.drop(['atc'],axis=1)\n",
    "X_snn = np.asarray(X_snn)\n",
    "result_snn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49373, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_snn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    }
   ],
   "source": [
    "x_train_snn, x_test_snn, y_train_snn, y_test_snn = train_test_split(X_snn,y_snn)\n",
    "\n",
    "x_train_flat_snn = x_train_snn.reshape(-1,16)\n",
    "x_test_flat_snn = x_test_snn.reshape(-1,16)\n",
    "\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds_snn = tsne.fit_transform(x_train_flat_snn[:500])\n",
    "scatter(train_tsne_embeds_snn, y_train_snn[:500], \"Samples from Training Data LINCS (DenseNet)\")\n",
    "\n",
    "eval_tsne_embeds_snn = tsne.fit_transform(x_test_flat_snn[:500])\n",
    "scatter(eval_tsne_embeds_snn, y_test_snn[:500], \"Samples from Validation Data LINCS (DenseNet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTriplet(x,y,testsize=0.2,ap_pairs=10,an_pairs=10):\n",
    "    data_xy = tuple([x,y])\n",
    "\n",
    "    trainsize = 1-testsize\n",
    "\n",
    "    triplet_train_pairs = []\n",
    "    triplet_test_pairs = []\n",
    "    for data_class in sorted(set(data_xy[1])):\n",
    "\n",
    "        same_class_idx = np.where((data_xy[1] == data_class))[0]\n",
    "        diff_class_idx = np.where(data_xy[1] != data_class)[0]\n",
    "        A_P_pairs = random.sample(list(permutations(same_class_idx,2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
    "        Neg_idx = random.sample(list(diff_class_idx),k=an_pairs)\n",
    "        \n",
    "\n",
    "        #train\n",
    "        A_P_len = len(A_P_pairs)\n",
    "        Neg_len = len(Neg_idx)\n",
    "        for ap in A_P_pairs[:int(A_P_len*trainsize)]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_train_pairs.append([Anchor,Positive,Negative])               \n",
    "        #test\n",
    "        for ap in A_P_pairs[int(A_P_len*trainsize):]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_test_pairs.append([Anchor,Positive,Negative])    \n",
    "                \n",
    "    return np.array(triplet_train_pairs), np.array(triplet_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_snn, X_test_snn = generateTriplet(X_snn[:50],y_snn[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "    print('y_pred.shape = ',y_pred)\n",
    "    \n",
    "    total_length = y_pred.shape.as_list()[-1]\n",
    "    \n",
    "    anchor = y_pred[:,:,0:14]\n",
    "    positive = y_pred[:,:,14:28]\n",
    "    negative = y_pred[:,:,28:]\n",
    "\n",
    "    print(anchor,'\\n', positive,'\\n', negative, '\\n',y_pred)\n",
    "    print(anchor.shape, positive.shape, negative.shape)\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss\n",
    "\n",
    "def baseNetwork():    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "        model.add(Dropout(0.9))\n",
    "        model.add(Dense(1000, activation='softmax'))\n",
    "        model.add(Dropout(0.9))\n",
    "        model.add(Dense(14, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape =  Tensor(\"merged_layer_18/concat:0\", shape=(?, 16, 42), dtype=float32)\n",
      "Tensor(\"loss_17/merged_layer_loss/strided_slice:0\", shape=(?, 16, 14), dtype=float32) \n",
      " Tensor(\"loss_17/merged_layer_loss/strided_slice_1:0\", shape=(?, 16, 14), dtype=float32) \n",
      " Tensor(\"loss_17/merged_layer_loss/strided_slice_2:0\", shape=(?, 16, 14), dtype=float32) \n",
      " Tensor(\"merged_layer_18/concat:0\", shape=(?, 16, 42), dtype=float32)\n",
      "(?, 16, 14) (?, 16, 14) (?, 16, 14)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 16, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 16, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 16, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 16, 14)       1017014     anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 16, 42)       0           sequential_19[1][0]              \n",
      "                                                                 sequential_19[2][0]              \n",
      "                                                                 sequential_19[3][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,017,014\n",
      "Trainable params: 1,017,014\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "anchor_input = Input((16,1, ), name='anchor_input')\n",
    "positive_input = Input((16,1, ), name='positive_input')\n",
    "negative_input = Input((16,1, ), name='negative_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "Shared_DNN = baseNetwork()\n",
    "encoded_anchor = Shared_DNN(anchor_input)\n",
    "encoded_positive = Shared_DNN(positive_input)\n",
    "encoded_negative = Shared_DNN(negative_input)\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative],axis=-1, name='merged_layer')\n",
    "\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "model.compile(loss=triplet_loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 5s 30ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 65/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.4000 - val_loss: 0.4000\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.4000 - val_loss: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a393ee828>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anchor_snn = X_train_snn[:,0,:].reshape(-1,16,1)\n",
    "Positive_snn = X_train_snn[:,1,:].reshape(-1,16,1)\n",
    "Negative_snn = X_train_snn[:,2,:].reshape(-1,16,1)\n",
    "Anchor_test_snn = X_test_snn[:,0,:].reshape(-1,16,1)\n",
    "Positive_test_snn = X_test_snn[:,1,:].reshape(-1,16,1)\n",
    "Negative_test_snn = X_test_snn[:,2,:].reshape(-1,16,1)\n",
    "\n",
    "Y_dummy = np.empty((Anchor_snn.shape[0],300))\n",
    "Y_dummy2 = np.empty((Anchor_test_snn.shape[0],1))\n",
    "\n",
    "model.fit([Anchor_snn,Positive_snn,Negative_snn],y=Y_dummy,validation_data=([Anchor_test_snn,Positive_test_snn,Negative_test_snn],Y_dummy2), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model = Model(inputs=anchor_input, outputs=encoded_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "X_train_trm = trained_model.predict(x_train_snn[:100].reshape(-1,16,1))\n",
    "X_test_trm = trained_model.predict(x_test_snn[:100].reshape(-1,16,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        ...,\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155155, 0.07131518]],\n",
       "\n",
       "       [[0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155155, 0.07131518],\n",
       "        ...,\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518]],\n",
       "\n",
       "       [[0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        ...,\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155155, 0.07131518]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        ...,\n",
       "        [0.071298  , 0.07152328, 0.07134152, ..., 0.07130028,\n",
       "         0.07155155, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518]],\n",
       "\n",
       "       [[0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        ...,\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518]],\n",
       "\n",
       "       [[0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        ...,\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134152, ..., 0.07130028,\n",
       "         0.07155156, 0.07131518],\n",
       "        [0.071298  , 0.07152329, 0.07134153, ..., 0.07130028,\n",
       "         0.07155156, 0.07131519]]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_trm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
