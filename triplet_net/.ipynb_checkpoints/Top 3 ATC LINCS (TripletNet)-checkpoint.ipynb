{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt1 \n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate, Conv1D,Conv2D, Flatten, Reshape, Embedding, GRU, SpatialDropout1D, LSTM, Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from itertools import permutations\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from scipy.stats import trim_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>inchi_key</th>\n",
       "      <th>atc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NC12CC3CC(CC(C3)C1)C2</td>\n",
       "      <td>amantadine</td>\n",
       "      <td>BRD-K70330367</td>\n",
       "      <td>DKNWSYNQZKUICI-UHFFFAOYSA-N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>CS(=O)(=O)OCCCCOS(C)(=O)=O</td>\n",
       "      <td>busulfan</td>\n",
       "      <td>BRD-K23204545</td>\n",
       "      <td>COVZYZSDYWQREU-UHFFFAOYSA-N</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>CCCC(C)(COC(N)=O)COC(=O)NC(C)C</td>\n",
       "      <td>carisoprodol</td>\n",
       "      <td>BRD-A99939097</td>\n",
       "      <td>OFZCIYFFPZCNJE-UHFFFAOYSA-N</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>CCN(CC)C(=O)N1CCN(C)CC1</td>\n",
       "      <td>diethylcarbamazine</td>\n",
       "      <td>BRD-K45542189</td>\n",
       "      <td>RCKMWOKWVGPNJF-UHFFFAOYSA-N</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>CCN(CC)C(=S)SSC(=S)N(CC)CC</td>\n",
       "      <td>disulfiram</td>\n",
       "      <td>BRD-K32744045</td>\n",
       "      <td>AUZONCFQVSMFAP-UHFFFAOYSA-N</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             smiles                name             id  \\\n",
       "75            NC12CC3CC(CC(C3)C1)C2          amantadine  BRD-K70330367   \n",
       "291      CS(=O)(=O)OCCCCOS(C)(=O)=O            busulfan  BRD-K23204545   \n",
       "322  CCCC(C)(COC(N)=O)COC(=O)NC(C)C        carisoprodol  BRD-A99939097   \n",
       "491         CCN(CC)C(=O)N1CCN(C)CC1  diethylcarbamazine  BRD-K45542189   \n",
       "510      CCN(CC)C(=S)SSC(=S)N(CC)CC          disulfiram  BRD-K32744045   \n",
       "\n",
       "                       inchi_key atc  \n",
       "75   DKNWSYNQZKUICI-UHFFFAOYSA-N   N  \n",
       "291  COVZYZSDYWQREU-UHFFFAOYSA-N   L  \n",
       "322  OFZCIYFFPZCNJE-UHFFFAOYSA-N   M  \n",
       "491  RCKMWOKWVGPNJF-UHFFFAOYSA-N   P  \n",
       "510  AUZONCFQVSMFAP-UHFFFAOYSA-N   P  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = pd.read_csv(\"../data/drug_class_identification/all3.csv\")\n",
    "full = full.dropna()\n",
    "full['atc'] = full['atc'].apply(lambda x : x[0])\n",
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our own plot function\n",
    "def scatter(x, y, subtitle=None):\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(y)\n",
    "\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 3))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(3):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = trim_mean(x[labels == i, :], axis=0, proportiontocut=0.2)\n",
    "        letter = le.inverse_transform([i])[0]\n",
    "        txt = ax.text(xtext, ytext, str(letter), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.savefig(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2170, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn = pd.read_csv('EMB_triplet_3_500_16_0.9_5-50')\n",
    "snn['id'] = snn['pert_id']\n",
    "snn1 = snn.groupby(['id']).mean()\n",
    "snn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>e10</th>\n",
       "      <th>e11</th>\n",
       "      <th>e12</th>\n",
       "      <th>e13</th>\n",
       "      <th>e14</th>\n",
       "      <th>e15</th>\n",
       "      <th>e16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRD-A00147595</th>\n",
       "      <td>20.5</td>\n",
       "      <td>-0.051912</td>\n",
       "      <td>-0.046225</td>\n",
       "      <td>-0.079341</td>\n",
       "      <td>0.010777</td>\n",
       "      <td>0.199179</td>\n",
       "      <td>0.063099</td>\n",
       "      <td>-0.008241</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.207154</td>\n",
       "      <td>-0.132532</td>\n",
       "      <td>-0.337864</td>\n",
       "      <td>-0.068388</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>0.483786</td>\n",
       "      <td>0.033555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRD-A00218260</th>\n",
       "      <td>62.5</td>\n",
       "      <td>-0.007211</td>\n",
       "      <td>-0.019740</td>\n",
       "      <td>-0.087918</td>\n",
       "      <td>0.043078</td>\n",
       "      <td>0.186389</td>\n",
       "      <td>0.050391</td>\n",
       "      <td>-0.042841</td>\n",
       "      <td>-0.038788</td>\n",
       "      <td>0.219889</td>\n",
       "      <td>-0.129041</td>\n",
       "      <td>-0.340753</td>\n",
       "      <td>-0.050279</td>\n",
       "      <td>0.034886</td>\n",
       "      <td>0.055037</td>\n",
       "      <td>0.480075</td>\n",
       "      <td>0.092194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRD-A00376169</th>\n",
       "      <td>104.5</td>\n",
       "      <td>-0.035873</td>\n",
       "      <td>-0.038781</td>\n",
       "      <td>-0.071005</td>\n",
       "      <td>0.011709</td>\n",
       "      <td>0.188347</td>\n",
       "      <td>0.062192</td>\n",
       "      <td>-0.015706</td>\n",
       "      <td>-0.001051</td>\n",
       "      <td>0.206982</td>\n",
       "      <td>-0.126873</td>\n",
       "      <td>-0.332761</td>\n",
       "      <td>-0.050283</td>\n",
       "      <td>0.053545</td>\n",
       "      <td>0.025172</td>\n",
       "      <td>0.478171</td>\n",
       "      <td>0.048873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRD-A00546892</th>\n",
       "      <td>146.5</td>\n",
       "      <td>-0.083619</td>\n",
       "      <td>-0.048509</td>\n",
       "      <td>-0.197024</td>\n",
       "      <td>0.099494</td>\n",
       "      <td>0.271578</td>\n",
       "      <td>0.030857</td>\n",
       "      <td>-0.031520</td>\n",
       "      <td>-0.126297</td>\n",
       "      <td>0.232526</td>\n",
       "      <td>-0.192449</td>\n",
       "      <td>-0.365771</td>\n",
       "      <td>-0.211621</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.019896</td>\n",
       "      <td>0.507215</td>\n",
       "      <td>0.036830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BRD-A00938334</th>\n",
       "      <td>188.5</td>\n",
       "      <td>-0.008202</td>\n",
       "      <td>-0.008061</td>\n",
       "      <td>-0.162835</td>\n",
       "      <td>0.113867</td>\n",
       "      <td>0.216934</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>-0.071992</td>\n",
       "      <td>-0.136495</td>\n",
       "      <td>0.225272</td>\n",
       "      <td>-0.158428</td>\n",
       "      <td>-0.337248</td>\n",
       "      <td>-0.135821</td>\n",
       "      <td>-0.009357</td>\n",
       "      <td>0.072566</td>\n",
       "      <td>0.464084</td>\n",
       "      <td>0.114936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Unnamed: 0        e1        e2        e3        e4        e5  \\\n",
       "id                                                                            \n",
       "BRD-A00147595        20.5 -0.051912 -0.046225 -0.079341  0.010777  0.199179   \n",
       "BRD-A00218260        62.5 -0.007211 -0.019740 -0.087918  0.043078  0.186389   \n",
       "BRD-A00376169       104.5 -0.035873 -0.038781 -0.071005  0.011709  0.188347   \n",
       "BRD-A00546892       146.5 -0.083619 -0.048509 -0.197024  0.099494  0.271578   \n",
       "BRD-A00938334       188.5 -0.008202 -0.008061 -0.162835  0.113867  0.216934   \n",
       "\n",
       "                     e6        e7        e8        e9       e10       e11  \\\n",
       "id                                                                          \n",
       "BRD-A00147595  0.063099 -0.008241 -0.000671  0.207154 -0.132532 -0.337864   \n",
       "BRD-A00218260  0.050391 -0.042841 -0.038788  0.219889 -0.129041 -0.340753   \n",
       "BRD-A00376169  0.062192 -0.015706 -0.001051  0.206982 -0.126873 -0.332761   \n",
       "BRD-A00546892  0.030857 -0.031520 -0.126297  0.232526 -0.192449 -0.365771   \n",
       "BRD-A00938334  0.018615 -0.071992 -0.136495  0.225272 -0.158428 -0.337248   \n",
       "\n",
       "                    e12       e13       e14       e15       e16  \n",
       "id                                                               \n",
       "BRD-A00147595 -0.068388  0.055600  0.015693  0.483786  0.033555  \n",
       "BRD-A00218260 -0.050279  0.034886  0.055037  0.480075  0.092194  \n",
       "BRD-A00376169 -0.050283  0.053545  0.025172  0.478171  0.048873  \n",
       "BRD-A00546892 -0.211621  0.006241  0.019896  0.507215  0.036830  \n",
       "BRD-A00938334 -0.135821 -0.009357  0.072566  0.464084  0.114936  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snn1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_snn = pd.merge(snn, full, how=\"inner\").drop(['id','smiles','name','inchi_key','pert_id','Unnamed: 0'],axis=1).dropna()\n",
    "result_snn = result_snn[result_snn.atc.isin(['C','L','N'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>e10</th>\n",
       "      <th>e11</th>\n",
       "      <th>e12</th>\n",
       "      <th>e13</th>\n",
       "      <th>e14</th>\n",
       "      <th>e15</th>\n",
       "      <th>e16</th>\n",
       "      <th>atc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.183448</td>\n",
       "      <td>-0.088374</td>\n",
       "      <td>-0.298852</td>\n",
       "      <td>0.141471</td>\n",
       "      <td>0.356243</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>-0.207486</td>\n",
       "      <td>0.229378</td>\n",
       "      <td>-0.251088</td>\n",
       "      <td>-0.377218</td>\n",
       "      <td>-0.378206</td>\n",
       "      <td>-0.015539</td>\n",
       "      <td>-0.035752</td>\n",
       "      <td>0.523691</td>\n",
       "      <td>-0.048028</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.185150</td>\n",
       "      <td>-0.092180</td>\n",
       "      <td>-0.293811</td>\n",
       "      <td>0.133686</td>\n",
       "      <td>0.357638</td>\n",
       "      <td>0.017342</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>-0.192717</td>\n",
       "      <td>0.231722</td>\n",
       "      <td>-0.250480</td>\n",
       "      <td>-0.383006</td>\n",
       "      <td>-0.374091</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>-0.038505</td>\n",
       "      <td>0.529485</td>\n",
       "      <td>-0.051879</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.255648</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>-0.080232</td>\n",
       "      <td>0.183721</td>\n",
       "      <td>0.105543</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>-0.235274</td>\n",
       "      <td>-0.185011</td>\n",
       "      <td>0.297990</td>\n",
       "      <td>-0.094100</td>\n",
       "      <td>-0.384150</td>\n",
       "      <td>0.106757</td>\n",
       "      <td>-0.053028</td>\n",
       "      <td>0.279614</td>\n",
       "      <td>0.506027</td>\n",
       "      <td>0.427559</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-0.130725</td>\n",
       "      <td>-0.069101</td>\n",
       "      <td>-0.291202</td>\n",
       "      <td>0.150245</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.016205</td>\n",
       "      <td>-0.031880</td>\n",
       "      <td>-0.201702</td>\n",
       "      <td>0.259922</td>\n",
       "      <td>-0.250814</td>\n",
       "      <td>-0.409143</td>\n",
       "      <td>-0.334909</td>\n",
       "      <td>-0.017611</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.547097</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.240981</td>\n",
       "      <td>0.119331</td>\n",
       "      <td>-0.100761</td>\n",
       "      <td>0.189807</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>-0.001084</td>\n",
       "      <td>-0.229800</td>\n",
       "      <td>-0.190443</td>\n",
       "      <td>0.297566</td>\n",
       "      <td>-0.106891</td>\n",
       "      <td>-0.392855</td>\n",
       "      <td>0.074723</td>\n",
       "      <td>-0.052318</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.520312</td>\n",
       "      <td>0.414156</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          e1        e2        e3        e4        e5        e6        e7  \\\n",
       "84 -0.183448 -0.088374 -0.298852  0.141471  0.356243  0.013249 -0.001803   \n",
       "85 -0.185150 -0.092180 -0.293811  0.133686  0.357638  0.017342  0.002558   \n",
       "86  0.255648  0.131313 -0.080232  0.183721  0.105543  0.000734 -0.235274   \n",
       "87 -0.130725 -0.069101 -0.291202  0.150245  0.345000  0.016205 -0.031880   \n",
       "88  0.240981  0.119331 -0.100761  0.189807  0.118800 -0.001084 -0.229800   \n",
       "\n",
       "          e8        e9       e10       e11       e12       e13       e14  \\\n",
       "84 -0.207486  0.229378 -0.251088 -0.377218 -0.378206 -0.015539 -0.035752   \n",
       "85 -0.192717  0.231722 -0.250480 -0.383006 -0.374091 -0.008439 -0.038505   \n",
       "86 -0.185011  0.297990 -0.094100 -0.384150  0.106757 -0.053028  0.279614   \n",
       "87 -0.201702  0.259922 -0.250814 -0.409143 -0.334909 -0.017611  0.002179   \n",
       "88 -0.190443  0.297566 -0.106891 -0.392855  0.074723 -0.052318  0.271000   \n",
       "\n",
       "         e15       e16 atc  \n",
       "84  0.523691 -0.048028   N  \n",
       "85  0.529485 -0.051879   N  \n",
       "86  0.506027  0.427559   N  \n",
       "87  0.547097  0.010761   N  \n",
       "88  0.520312  0.414156   N  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_snn = result_snn['atc']\n",
    "X_snn = result_snn.drop(['atc'],axis=1)\n",
    "X_snn = np.asarray(X_snn)\n",
    "result_snn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_snn.shape\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(y_snn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    }
   ],
   "source": [
    "x_train_snn, x_test_snn, y_train_snn, y_test_snn = train_test_split(X_snn,labels)\n",
    "\n",
    "x_train_flat_snn = x_train_snn.reshape(-1,16)\n",
    "x_test_flat_snn = x_test_snn.reshape(-1,16)\n",
    "\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds_snn = tsne.fit_transform(x_train_flat_snn[:1000])\n",
    "scatter(train_tsne_embeds_snn, y_train_snn[:1000], \"Samples from Training Data LINCS (Triplet)\")\n",
    "\n",
    "eval_tsne_embeds_snn = tsne.fit_transform(x_test_flat_snn[:1000])\n",
    "scatter(eval_tsne_embeds_snn, y_test_snn[:1000], \"Samples from Validation Data LINCS (Triplet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseNetwork(op):    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, activation='softmax', input_dim=16))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(20, activation='softmax'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(op, activation='softmax'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                850       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 2,113\n",
      "Trainable params: 2,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = baseNetwork(3)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18819/18819 [==============================] - ETA: 0s - loss: 1.0551 - acc: 0.475 - 2s 114us/step - loss: 1.0551 - acc: 0.4758\n",
      "Epoch 2/1000\n",
      "18819/18819 [==============================] - 1s 75us/step - loss: 1.0395 - acc: 0.5067\n",
      "Epoch 3/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0362 - acc: 0.5067\n",
      "Epoch 4/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0332 - acc: 0.5067\n",
      "Epoch 5/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0313 - acc: 0.5067\n",
      "Epoch 6/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0288 - acc: 0.5067: 0s - loss: 1.0303 - a\n",
      "Epoch 7/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0256 - acc: 0.5067\n",
      "Epoch 8/1000\n",
      "18819/18819 [==============================] - 1s 76us/step - loss: 1.0229 - acc: 0.5067\n",
      "Epoch 9/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0209 - acc: 0.5067\n",
      "Epoch 10/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0189 - acc: 0.5067\n",
      "Epoch 11/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0177 - acc: 0.5067\n",
      "Epoch 12/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0174 - acc: 0.5067\n",
      "Epoch 13/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0168 - acc: 0.5067\n",
      "Epoch 14/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0166 - acc: 0.5067\n",
      "Epoch 15/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0158 - acc: 0.5067\n",
      "Epoch 16/1000\n",
      "18819/18819 [==============================] - 1s 76us/step - loss: 1.0156 - acc: 0.5067\n",
      "Epoch 17/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0151 - acc: 0.5067\n",
      "Epoch 18/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0136 - acc: 0.5067\n",
      "Epoch 19/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0124 - acc: 0.5067\n",
      "Epoch 20/1000\n",
      "18819/18819 [==============================] - 2s 89us/step - loss: 1.0148 - acc: 0.5067\n",
      "Epoch 21/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0147 - acc: 0.5067\n",
      "Epoch 22/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0115 - acc: 0.5067\n",
      "Epoch 23/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0121 - acc: 0.5067\n",
      "Epoch 24/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0121 - acc: 0.5067: 1\n",
      "Epoch 25/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0109 - acc: 0.5067\n",
      "Epoch 26/1000\n",
      "18819/18819 [==============================] - 2s 88us/step - loss: 1.0123 - acc: 0.5067\n",
      "Epoch 27/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0088 - acc: 0.5067\n",
      "Epoch 28/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0098 - acc: 0.5067\n",
      "Epoch 29/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0094 - acc: 0.5067\n",
      "Epoch 30/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0098 - acc: 0.5067\n",
      "Epoch 31/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0091 - acc: 0.5067\n",
      "Epoch 32/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0084 - acc: 0.5067\n",
      "Epoch 33/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0097 - acc: 0.5067\n",
      "Epoch 34/1000\n",
      "18819/18819 [==============================] - 2s 91us/step - loss: 1.0087 - acc: 0.5067\n",
      "Epoch 35/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0088 - acc: 0.5067\n",
      "Epoch 36/1000\n",
      "18819/18819 [==============================] - 2s 88us/step - loss: 1.0085 - acc: 0.5067\n",
      "Epoch 37/1000\n",
      "18819/18819 [==============================] - ETA: 0s - loss: 1.0080 - acc: 0.506 - 2s 84us/step - loss: 1.0086 - acc: 0.5067\n",
      "Epoch 38/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0090 - acc: 0.5067\n",
      "Epoch 39/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0075 - acc: 0.5067\n",
      "Epoch 40/1000\n",
      "18819/18819 [==============================] - 2s 93us/step - loss: 1.0086 - acc: 0.5067\n",
      "Epoch 41/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0075 - acc: 0.5067\n",
      "Epoch 42/1000\n",
      "18819/18819 [==============================] - 2s 91us/step - loss: 1.0088 - acc: 0.5067\n",
      "Epoch 43/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0081 - acc: 0.5067\n",
      "Epoch 44/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0076 - acc: 0.5067\n",
      "Epoch 45/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0074 - acc: 0.5067\n",
      "Epoch 46/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0075 - acc: 0.5067\n",
      "Epoch 47/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0080 - acc: 0.5067\n",
      "Epoch 48/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0070 - acc: 0.5067\n",
      "Epoch 49/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 50/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0069 - acc: 0.5067\n",
      "Epoch 51/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0073 - acc: 0.5067\n",
      "Epoch 52/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0066 - acc: 0.5067\n",
      "Epoch 53/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0089 - acc: 0.5067\n",
      "Epoch 54/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0063 - acc: 0.5067\n",
      "Epoch 55/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0084 - acc: 0.5067\n",
      "Epoch 56/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0078 - acc: 0.5067\n",
      "Epoch 57/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0069 - acc: 0.5067\n",
      "Epoch 58/1000\n",
      "18819/18819 [==============================] - 2s 90us/step - loss: 1.0070 - acc: 0.5067: 2\n",
      "Epoch 59/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0076 - acc: 0.5067\n",
      "Epoch 60/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0067 - acc: 0.5067\n",
      "Epoch 61/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0061 - acc: 0.5067\n",
      "Epoch 62/1000\n",
      "18819/18819 [==============================] - 2s 89us/step - loss: 1.0065 - acc: 0.5067\n",
      "Epoch 63/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0069 - acc: 0.5067\n",
      "Epoch 64/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0069 - acc: 0.5067\n",
      "Epoch 65/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0069 - acc: 0.5067\n",
      "Epoch 66/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0062 - acc: 0.5067\n",
      "Epoch 67/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0068 - acc: 0.5067\n",
      "Epoch 68/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 69/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0068 - acc: 0.5067\n",
      "Epoch 70/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0065 - acc: 0.5067\n",
      "Epoch 71/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0053 - acc: 0.5067\n",
      "Epoch 72/1000\n",
      "18819/18819 [==============================] - 2s 89us/step - loss: 1.0070 - acc: 0.5067\n",
      "Epoch 73/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0070 - acc: 0.5067\n",
      "Epoch 74/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0067 - acc: 0.5067\n",
      "Epoch 75/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 76/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0076 - acc: 0.5067\n",
      "Epoch 77/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0053 - acc: 0.5067\n",
      "Epoch 78/1000\n",
      "18819/18819 [==============================] - 2s 91us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0062 - acc: 0.5067\n",
      "Epoch 80/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0065 - acc: 0.5067\n",
      "Epoch 81/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0056 - acc: 0.5067\n",
      "Epoch 82/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0060 - acc: 0.5067\n",
      "Epoch 83/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0062 - acc: 0.5066\n",
      "Epoch 84/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0068 - acc: 0.5067\n",
      "Epoch 85/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 86/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0059 - acc: 0.5067\n",
      "Epoch 87/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0074 - acc: 0.5067\n",
      "Epoch 88/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0058 - acc: 0.5067\n",
      "Epoch 89/1000\n",
      "18819/18819 [==============================] - 3s 144us/step - loss: 1.0070 - acc: 0.5067\n",
      "Epoch 90/1000\n",
      "18819/18819 [==============================] - 2s 127us/step - loss: 1.0049 - acc: 0.5067 1s - los\n",
      "Epoch 91/1000\n",
      "18819/18819 [==============================] - 3s 152us/step - loss: 1.0055 - acc: 0.5067\n",
      "Epoch 92/1000\n",
      "18819/18819 [==============================] - 2s 121us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 93/1000\n",
      "18819/18819 [==============================] - 2s 110us/step - loss: 1.0062 - acc: 0.5067\n",
      "Epoch 94/1000\n",
      "18819/18819 [==============================] - 2s 122us/step - loss: 1.0069 - acc: 0.5067\n",
      "Epoch 95/1000\n",
      "18819/18819 [==============================] - 2s 91us/step - loss: 1.0065 - acc: 0.5066\n",
      "Epoch 96/1000\n",
      "18819/18819 [==============================] - 2s 119us/step - loss: 1.0053 - acc: 0.5066\n",
      "Epoch 97/1000\n",
      "18819/18819 [==============================] - 2s 105us/step - loss: 1.0053 - acc: 0.5067\n",
      "Epoch 98/1000\n",
      "18819/18819 [==============================] - 2s 118us/step - loss: 1.0059 - acc: 0.5067\n",
      "Epoch 99/1000\n",
      "18819/18819 [==============================] - 1s 76us/step - loss: 1.0054 - acc: 0.5066\n",
      "Epoch 100/1000\n",
      "18819/18819 [==============================] - 2s 93us/step - loss: 1.0053 - acc: 0.5067\n",
      "Epoch 101/1000\n",
      "18819/18819 [==============================] - 3s 134us/step - loss: 1.0047 - acc: 0.5067\n",
      "Epoch 102/1000\n",
      "18819/18819 [==============================] - 2s 126us/step - loss: 1.0060 - acc: 0.5068\n",
      "Epoch 103/1000\n",
      "18819/18819 [==============================] - 3s 146us/step - loss: 1.0051 - acc: 0.5066\n",
      "Epoch 104/1000\n",
      "18819/18819 [==============================] - 2s 113us/step - loss: 1.0057 - acc: 0.5066\n",
      "Epoch 105/1000\n",
      "18819/18819 [==============================] - 2s 125us/step - loss: 1.0053 - acc: 0.5065\n",
      "Epoch 106/1000\n",
      "18819/18819 [==============================] - 3s 134us/step - loss: 1.0071 - acc: 0.5067\n",
      "Epoch 107/1000\n",
      "18819/18819 [==============================] - 3s 138us/step - loss: 1.0052 - acc: 0.5067\n",
      "Epoch 108/1000\n",
      "18819/18819 [==============================] - 2s 114us/step - loss: 1.0080 - acc: 0.5065\n",
      "Epoch 109/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0066 - acc: 0.5067\n",
      "Epoch 110/1000\n",
      "18819/18819 [==============================] - 2s 90us/step - loss: 1.0065 - acc: 0.5066\n",
      "Epoch 111/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0046 - acc: 0.5067\n",
      "Epoch 112/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0061 - acc: 0.5070\n",
      "Epoch 113/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0048 - acc: 0.5067\n",
      "Epoch 114/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0060 - acc: 0.5067\n",
      "Epoch 115/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0052 - acc: 0.5067\n",
      "Epoch 116/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0055 - acc: 0.5067\n",
      "Epoch 117/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0046 - acc: 0.5067\n",
      "Epoch 118/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0050 - acc: 0.5068\n",
      "Epoch 119/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0054 - acc: 0.5067\n",
      "Epoch 120/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0059 - acc: 0.5066\n",
      "Epoch 121/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0051 - acc: 0.5067\n",
      "Epoch 122/1000\n",
      "18819/18819 [==============================] - 2s 92us/step - loss: 1.0047 - acc: 0.5067\n",
      "Epoch 123/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0058 - acc: 0.5065\n",
      "Epoch 124/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0051 - acc: 0.5064\n",
      "Epoch 125/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0052 - acc: 0.5067\n",
      "Epoch 126/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0056 - acc: 0.5066\n",
      "Epoch 127/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0052 - acc: 0.5068: 1s -\n",
      "Epoch 128/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0044 - acc: 0.5068\n",
      "Epoch 129/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0034 - acc: 0.5067\n",
      "Epoch 130/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0055 - acc: 0.5064\n",
      "Epoch 131/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0047 - acc: 0.5065\n",
      "Epoch 132/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0038 - acc: 0.5063\n",
      "Epoch 133/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0048 - acc: 0.5069\n",
      "Epoch 134/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0061 - acc: 0.5064\n",
      "Epoch 135/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0047 - acc: 0.5067\n",
      "Epoch 136/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0045 - acc: 0.5069\n",
      "Epoch 137/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0043 - acc: 0.5063\n",
      "Epoch 138/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0047 - acc: 0.5060: 1s -\n",
      "Epoch 139/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0059 - acc: 0.5067\n",
      "Epoch 140/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0047 - acc: 0.5065\n",
      "Epoch 141/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0058 - acc: 0.5068\n",
      "Epoch 142/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0049 - acc: 0.5063\n",
      "Epoch 143/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0057 - acc: 0.5064\n",
      "Epoch 144/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0044 - acc: 0.5066\n",
      "Epoch 145/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0050 - acc: 0.5067\n",
      "Epoch 146/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0059 - acc: 0.5066\n",
      "Epoch 147/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0046 - acc: 0.5072\n",
      "Epoch 148/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0058 - acc: 0.5061\n",
      "Epoch 149/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0042 - acc: 0.5068\n",
      "Epoch 150/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0044 - acc: 0.5067\n",
      "Epoch 151/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0051 - acc: 0.5067\n",
      "Epoch 152/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0050 - acc: 0.5065\n",
      "Epoch 153/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0054 - acc: 0.5065\n",
      "Epoch 154/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0037 - acc: 0.5065: 0s - loss: 1.0024 - acc: 0\n",
      "Epoch 155/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0050 - acc: 0.5065\n",
      "Epoch 156/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0051 - acc: 0.5066\n",
      "Epoch 157/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0055 - acc: 0.5065\n",
      "Epoch 158/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0051 - acc: 0.5063\n",
      "Epoch 159/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0054 - acc: 0.5066\n",
      "Epoch 160/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0044 - acc: 0.5066\n",
      "Epoch 161/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0037 - acc: 0.5065\n",
      "Epoch 162/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0053 - acc: 0.5067\n",
      "Epoch 163/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0041 - acc: 0.5066\n",
      "Epoch 164/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0044 - acc: 0.5064\n",
      "Epoch 165/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0051 - acc: 0.5066\n",
      "Epoch 166/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0044 - acc: 0.5065\n",
      "Epoch 167/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0042 - acc: 0.5065\n",
      "Epoch 168/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0035 - acc: 0.5067\n",
      "Epoch 169/1000\n",
      "18819/18819 [==============================] - 1s 76us/step - loss: 1.0029 - acc: 0.5066\n",
      "Epoch 170/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0054 - acc: 0.5066: 1\n",
      "Epoch 171/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0047 - acc: 0.5067\n",
      "Epoch 172/1000\n",
      "18819/18819 [==============================] - 1s 75us/step - loss: 1.0046 - acc: 0.5065\n",
      "Epoch 173/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0034 - acc: 0.5067\n",
      "Epoch 174/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0037 - acc: 0.5068\n",
      "Epoch 175/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0044 - acc: 0.5067\n",
      "Epoch 176/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0042 - acc: 0.5067\n",
      "Epoch 177/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0046 - acc: 0.5067\n",
      "Epoch 178/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0033 - acc: 0.5067\n",
      "Epoch 179/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0053 - acc: 0.5065\n",
      "Epoch 180/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0048 - acc: 0.5066\n",
      "Epoch 181/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0025 - acc: 0.5066\n",
      "Epoch 182/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0044 - acc: 0.5064\n",
      "Epoch 183/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0034 - acc: 0.5067\n",
      "Epoch 184/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0038 - acc: 0.5062\n",
      "Epoch 185/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0036 - acc: 0.5060\n",
      "Epoch 186/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0036 - acc: 0.5065\n",
      "Epoch 187/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0037 - acc: 0.5064\n",
      "Epoch 188/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0040 - acc: 0.5061\n",
      "Epoch 189/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0035 - acc: 0.5062\n",
      "Epoch 190/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0037 - acc: 0.5067\n",
      "Epoch 191/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0033 - acc: 0.5062\n",
      "Epoch 192/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0047 - acc: 0.5066\n",
      "Epoch 193/1000\n",
      "18819/18819 [==============================] - 2s 92us/step - loss: 1.0043 - acc: 0.5068\n",
      "Epoch 194/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0044 - acc: 0.5056\n",
      "Epoch 195/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0037 - acc: 0.5067\n",
      "Epoch 196/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0045 - acc: 0.5063\n",
      "Epoch 197/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0031 - acc: 0.5069\n",
      "Epoch 198/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0020 - acc: 0.5060\n",
      "Epoch 199/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0042 - acc: 0.5068\n",
      "Epoch 200/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0029 - acc: 0.5070\n",
      "Epoch 201/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0039 - acc: 0.5063\n",
      "Epoch 202/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0041 - acc: 0.5059\n",
      "Epoch 203/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0050 - acc: 0.5073\n",
      "Epoch 204/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0040 - acc: 0.5065\n",
      "Epoch 205/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0047 - acc: 0.5065\n",
      "Epoch 206/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0032 - acc: 0.5067\n",
      "Epoch 207/1000\n",
      "18819/18819 [==============================] - 2s 90us/step - loss: 1.0040 - acc: 0.5057\n",
      "Epoch 208/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0046 - acc: 0.5052\n",
      "Epoch 209/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0046 - acc: 0.5058\n",
      "Epoch 210/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0034 - acc: 0.5069\n",
      "Epoch 211/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0037 - acc: 0.5062\n",
      "Epoch 212/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0037 - acc: 0.5061\n",
      "Epoch 213/1000\n",
      "18819/18819 [==============================] - 2s 90us/step - loss: 1.0048 - acc: 0.5062\n",
      "Epoch 214/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0029 - acc: 0.5069\n",
      "Epoch 215/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0025 - acc: 0.5067\n",
      "Epoch 216/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0036 - acc: 0.5065\n",
      "Epoch 217/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0026 - acc: 0.5067\n",
      "Epoch 218/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0035 - acc: 0.5061\n",
      "Epoch 219/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0038 - acc: 0.5053\n",
      "Epoch 220/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0045 - acc: 0.5059\n",
      "Epoch 221/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0034 - acc: 0.5067\n",
      "Epoch 222/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0041 - acc: 0.5063\n",
      "Epoch 223/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0038 - acc: 0.5061\n",
      "Epoch 224/1000\n",
      "18819/18819 [==============================] - 1s 76us/step - loss: 1.0052 - acc: 0.5067\n",
      "Epoch 225/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0037 - acc: 0.5064\n",
      "Epoch 226/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0036 - acc: 0.5061\n",
      "Epoch 227/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0039 - acc: 0.5057\n",
      "Epoch 228/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0033 - acc: 0.5066\n",
      "Epoch 229/1000\n",
      "18819/18819 [==============================] - 2s 88us/step - loss: 1.0027 - acc: 0.5061\n",
      "Epoch 230/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0026 - acc: 0.5064\n",
      "Epoch 231/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0036 - acc: 0.5061\n",
      "Epoch 232/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0046 - acc: 0.5059\n",
      "Epoch 233/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0034 - acc: 0.5065\n",
      "Epoch 234/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0032 - acc: 0.5068\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0027 - acc: 0.5067\n",
      "Epoch 236/1000\n",
      "18819/18819 [==============================] - 2s 90us/step - loss: 1.0039 - acc: 0.5065: 1s \n",
      "Epoch 237/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0028 - acc: 0.5068\n",
      "Epoch 238/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0033 - acc: 0.5062\n",
      "Epoch 239/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0032 - acc: 0.5062\n",
      "Epoch 240/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0038 - acc: 0.5061\n",
      "Epoch 241/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0042 - acc: 0.5065\n",
      "Epoch 242/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0042 - acc: 0.5060\n",
      "Epoch 243/1000\n",
      "18819/18819 [==============================] - 2s 88us/step - loss: 1.0039 - acc: 0.5066\n",
      "Epoch 244/1000\n",
      "18819/18819 [==============================] - 2s 89us/step - loss: 1.0034 - acc: 0.5063\n",
      "Epoch 245/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0033 - acc: 0.5061\n",
      "Epoch 246/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0039 - acc: 0.5066\n",
      "Epoch 247/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0040 - acc: 0.5066\n",
      "Epoch 248/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0022 - acc: 0.5068\n",
      "Epoch 249/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0046 - acc: 0.5065\n",
      "Epoch 250/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0041 - acc: 0.5069\n",
      "Epoch 251/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0034 - acc: 0.5062\n",
      "Epoch 252/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0045 - acc: 0.5051\n",
      "Epoch 253/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0045 - acc: 0.5068\n",
      "Epoch 254/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0033 - acc: 0.5068\n",
      "Epoch 255/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0043 - acc: 0.5061\n",
      "Epoch 256/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0023 - acc: 0.5061\n",
      "Epoch 257/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0024 - acc: 0.5067\n",
      "Epoch 258/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0032 - acc: 0.5059\n",
      "Epoch 259/1000\n",
      "18819/18819 [==============================] - 1s 75us/step - loss: 1.0034 - acc: 0.5060\n",
      "Epoch 260/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0039 - acc: 0.5067\n",
      "Epoch 261/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0019 - acc: 0.5060\n",
      "Epoch 262/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0042 - acc: 0.5067\n",
      "Epoch 263/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0026 - acc: 0.5061\n",
      "Epoch 264/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0046 - acc: 0.5063\n",
      "Epoch 265/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0033 - acc: 0.5065\n",
      "Epoch 266/1000\n",
      "18819/18819 [==============================] - 1s 80us/step - loss: 1.0032 - acc: 0.5066\n",
      "Epoch 267/1000\n",
      "18819/18819 [==============================] - 2s 84us/step - loss: 1.0032 - acc: 0.5055\n",
      "Epoch 268/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0031 - acc: 0.5068\n",
      "Epoch 269/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0028 - acc: 0.5066\n",
      "Epoch 270/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0031 - acc: 0.5066\n",
      "Epoch 271/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0034 - acc: 0.5067\n",
      "Epoch 272/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0033 - acc: 0.5067\n",
      "Epoch 273/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0022 - acc: 0.5067\n",
      "Epoch 274/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0024 - acc: 0.5057\n",
      "Epoch 275/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0020 - acc: 0.5065\n",
      "Epoch 276/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0040 - acc: 0.5065\n",
      "Epoch 277/1000\n",
      "18819/18819 [==============================] - 2s 85us/step - loss: 1.0025 - acc: 0.5063\n",
      "Epoch 278/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0035 - acc: 0.5065\n",
      "Epoch 279/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0042 - acc: 0.5063\n",
      "Epoch 280/1000\n",
      "18819/18819 [==============================] - 2s 91us/step - loss: 1.0040 - acc: 0.5068\n",
      "Epoch 281/1000\n",
      "18819/18819 [==============================] - 2s 97us/step - loss: 1.0020 - acc: 0.5066\n",
      "Epoch 282/1000\n",
      "18819/18819 [==============================] - 2s 96us/step - loss: 1.0038 - acc: 0.5067\n",
      "Epoch 283/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0031 - acc: 0.5065\n",
      "Epoch 284/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0041 - acc: 0.5068\n",
      "Epoch 285/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0044 - acc: 0.5066\n",
      "Epoch 286/1000\n",
      "18819/18819 [==============================] - 2s 86us/step - loss: 1.0029 - acc: 0.5063\n",
      "Epoch 287/1000\n",
      "18819/18819 [==============================] - ETA: 0s - loss: 1.0013 - acc: 0.506 - 2s 93us/step - loss: 1.0018 - acc: 0.5063\n",
      "Epoch 288/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0029 - acc: 0.5066\n",
      "Epoch 289/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0024 - acc: 0.5066\n",
      "Epoch 290/1000\n",
      "18819/18819 [==============================] - 2s 88us/step - loss: 1.0033 - acc: 0.5063\n",
      "Epoch 291/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0025 - acc: 0.5065\n",
      "Epoch 292/1000\n",
      "18819/18819 [==============================] - 2s 95us/step - loss: 1.0028 - acc: 0.5059\n",
      "Epoch 293/1000\n",
      "18819/18819 [==============================] - 2s 98us/step - loss: 1.0031 - acc: 0.5066\n",
      "Epoch 294/1000\n",
      "18819/18819 [==============================] - 2s 89us/step - loss: 1.0026 - acc: 0.5065\n",
      "Epoch 295/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0017 - acc: 0.5058\n",
      "Epoch 296/1000\n",
      "18819/18819 [==============================] - 2s 87us/step - loss: 1.0035 - acc: 0.5066\n",
      "Epoch 297/1000\n",
      "18819/18819 [==============================] - 2s 92us/step - loss: 1.0010 - acc: 0.5067\n",
      "Epoch 298/1000\n",
      "18819/18819 [==============================] - 2s 89us/step - loss: 1.0033 - acc: 0.5060\n",
      "Epoch 299/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0020 - acc: 0.5060\n",
      "Epoch 300/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0029 - acc: 0.5068\n",
      "Epoch 301/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0038 - acc: 0.5066\n",
      "Epoch 302/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0029 - acc: 0.5068\n",
      "Epoch 303/1000\n",
      "18819/18819 [==============================] - 1s 79us/step - loss: 1.0025 - acc: 0.5056\n",
      "Epoch 304/1000\n",
      "18819/18819 [==============================] - 2s 81us/step - loss: 1.0023 - acc: 0.5061\n",
      "Epoch 305/1000\n",
      "18819/18819 [==============================] - 2s 83us/step - loss: 1.0032 - acc: 0.5060\n",
      "Epoch 306/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0034 - acc: 0.5061\n",
      "Epoch 307/1000\n",
      "18819/18819 [==============================] - 1s 78us/step - loss: 1.0023 - acc: 0.5057\n",
      "Epoch 308/1000\n",
      "18819/18819 [==============================] - 1s 77us/step - loss: 1.0020 - acc: 0.5071\n",
      "Epoch 309/1000\n",
      "18819/18819 [==============================] - 2s 82us/step - loss: 1.0013 - acc: 0.5050\n",
      "Epoch 310/1000\n",
      "18819/18819 [==============================] - ETA: 0s - loss: 1.0020 - acc: 0.506 - 3s 133us/step - loss: 1.0020 - acc: 0.5068\n",
      "Epoch 311/1000\n",
      "18819/18819 [==============================] - 2s 124us/step - loss: 1.0031 - acc: 0.5062\n",
      "Epoch 312/1000\n",
      "18819/18819 [==============================] - 3s 143us/step - loss: 1.0028 - acc: 0.5067\n",
      "Epoch 313/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18819/18819 [==============================] - 3s 134us/step - loss: 1.0018 - acc: 0.5051\n",
      "Epoch 314/1000\n",
      "18819/18819 [==============================] - 2s 80us/step - loss: 1.0028 - acc: 0.5069\n",
      "Epoch 315/1000\n",
      "15584/18819 [=======================>......] - ETA: 0s - loss: 1.0027 - acc: 0.5045"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5353f5286def>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_snn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_snn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    119\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3334\u001b[0m     \"\"\"\n\u001b[1;32m   3335\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3336\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3388\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3389\u001b[0m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3391\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3392\u001b[0m         \u001b[0;31m# if there are no nans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36m_median_nancheck\u001b[0;34m(data, result, axis, out)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misMaskedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             warnings.warn(\"Invalid value encountered in median\",\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train_snn,y_train_snn, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6274/6274 [==============================] - 0s 42us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9825418005903169, 0.5202422696844119]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_snn, y_test_snn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "inp = model.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in model.layers]          # all layer outputs\n",
    "functor = K.function([inp, K.learning_phase()], outputs ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_outs_train = functor([x_train_snn, 1.])\n",
    "layer_outs_test = functor([x_test_snn, 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    }
   ],
   "source": [
    "x_train_flat_snn = np.asarray(layer_outs_train[4]).reshape(-1,10)\n",
    "x_test_flat_snn = np.asarray(layer_outs_test[4]).reshape(-1,10)\n",
    "\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds_snn = tsne.fit_transform(x_train_flat_snn[:1000])\n",
    "scatter(train_tsne_embeds_snn, y_train_snn[:1000], \"Samples from Training Data LINCS (Triplet) after\")\n",
    "\n",
    "eval_tsne_embeds_snn = tsne.fit_transform(x_test_flat_snn[:1000])\n",
    "scatter(eval_tsne_embeds_snn, y_test_snn[:1000], \"Samples from Validation Data LINCS (Triplet) after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_trm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
