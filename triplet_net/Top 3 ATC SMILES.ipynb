{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt1 \n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate, Conv1D,Conv2D, Flatten, Reshape, Embedding, GRU, SpatialDropout1D, LSTM, Dropout, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from itertools import permutations\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from scipy.stats import trim_mean\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = pd.read_csv(\"../data/drug_class_identification/all3.csv\")\n",
    "full = full.dropna()\n",
    "full['atc'] = full['atc'].apply(lambda x : x[0])\n",
    "full = full[full.atc.isin(['C','L','N'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = full[\"smiles\"]\n",
    "y = full['atc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getVocabulary(sample):\n",
    "    vocabulary = set()\n",
    "    for word in sample:\n",
    "        for character in word:\n",
    "            vocabulary.add(character)\n",
    "    return (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "characters = getVocabulary(X)\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "samples = X.tolist()\n",
    "max_length = 70\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample[:max_length]):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422, 70, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(results)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our own plot function\n",
    "def scatter(x, y, subtitle=None):\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(y)\n",
    "\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 3))\n",
    "\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[labels.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    for i in range(3):\n",
    "        # Position of each label.\n",
    "        xtext, ytext = trim_mean(x[labels == i, :], axis=0, proportiontocut=0.2)\n",
    "        letter = le.inverse_transform([i])[0]\n",
    "        txt = ax.text(xtext, ytext, str(letter), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "        \n",
    "    if subtitle != None:\n",
    "        plt.suptitle(subtitle)\n",
    "        \n",
    "    plt.savefig(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 70, 30)\n",
      "(316, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test,y_train,y_test = train_test_split(X,y)\n",
    "print(x_train.shape)\n",
    "x_train_flat = np.asarray([sum(x_train[0]) for i in x_train]) #.reshape(-1,70*30)\n",
    "x_test_flat = np.asarray([sum(x_test[0]) for i in x_test])\n",
    "\n",
    "print(x_train_flat.shape)\n",
    "tsne = TSNE()\n",
    "train_tsne_embeds = tsne.fit_transform(x_train_flat)\n",
    "scatter(train_tsne_embeds, y_train, \"Samples from Training Data SMILES\")\n",
    "\n",
    "eval_tsne_embeds = tsne.fit_transform(x_test_flat)\n",
    "scatter(eval_tsne_embeds, y_test, \"Samples from Validation Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09352826, -0.04567604],\n",
       "       [-0.0928271 , -0.0442378 ],\n",
       "       [-0.09357231, -0.04429039],\n",
       "       [-0.09352819, -0.04567608],\n",
       "       [-0.09402925, -0.04471768],\n",
       "       [-0.09266791, -0.04423929],\n",
       "       [-0.09357221, -0.04429034],\n",
       "       [-0.09377973, -0.04423492],\n",
       "       [-0.09402936, -0.04471768],\n",
       "       [-0.09413906, -0.04423895],\n",
       "       [-0.09357221, -0.04429037],\n",
       "       [-0.09377959, -0.04423487],\n",
       "       [-0.09352828, -0.045676  ],\n",
       "       [-0.09357221, -0.04429042],\n",
       "       [-0.09131569, -0.04439673],\n",
       "       [-0.09357227, -0.04429042],\n",
       "       [-0.09357224, -0.04429039],\n",
       "       [-0.09357228, -0.04429043],\n",
       "       [-0.09262706, -0.04423918],\n",
       "       [-0.09357224, -0.04429038],\n",
       "       [-0.09352834, -0.04567604],\n",
       "       [-0.09357223, -0.04429043],\n",
       "       [-0.09391126, -0.04423662],\n",
       "       [-0.09470391, -0.04422791],\n",
       "       [-0.09357227, -0.04429033],\n",
       "       [-0.09357222, -0.0442904 ],\n",
       "       [-0.09357414, -0.04401596],\n",
       "       [-0.09281117, -0.04423763],\n",
       "       [-0.09352826, -0.045676  ],\n",
       "       [-0.0935741 , -0.04401595],\n",
       "       [-0.09357233, -0.0442904 ],\n",
       "       [-0.09357207, -0.04429042],\n",
       "       [-0.09357218, -0.04429039],\n",
       "       [-0.09357408, -0.04401593],\n",
       "       [-0.09357414, -0.04401592],\n",
       "       [-0.09357221, -0.04429046],\n",
       "       [-0.09558327, -0.04457737],\n",
       "       [-0.09357224, -0.04429037],\n",
       "       [-0.09391747, -0.04423663],\n",
       "       [-0.09352829, -0.04567605],\n",
       "       [-0.09429649, -0.0442601 ],\n",
       "       [-0.09357224, -0.04429037],\n",
       "       [-0.09352826, -0.04567606],\n",
       "       [-0.09389125, -0.04423647],\n",
       "       [-0.0935722 , -0.04429039],\n",
       "       [-0.09357227, -0.04429044],\n",
       "       [-0.09357225, -0.0442904 ],\n",
       "       [-0.09402925, -0.04471768],\n",
       "       [-0.09352823, -0.04567601],\n",
       "       [-0.09357228, -0.04429043],\n",
       "       [-0.09357228, -0.04429044],\n",
       "       [-0.09359546, -0.02841504],\n",
       "       [-0.09490549, -0.0442264 ],\n",
       "       [-0.09352827, -0.04567601],\n",
       "       [-0.09357227, -0.04429045],\n",
       "       [-0.09388725, -0.04423644],\n",
       "       [-0.0935723 , -0.04429048],\n",
       "       [-0.09285135, -0.04423795],\n",
       "       [-0.0935721 , -0.04429043],\n",
       "       [-0.09357221, -0.04429041],\n",
       "       [-0.0935722 , -0.04429042],\n",
       "       [-0.09352831, -0.04567602],\n",
       "       [-0.09516143, -0.04473124],\n",
       "       [-0.0941948 , -0.04423611],\n",
       "       [-0.0939222 , -0.04423673],\n",
       "       [-0.09794492, -0.04460243],\n",
       "       [-0.09357211, -0.04429045],\n",
       "       [-0.09402926, -0.04471767],\n",
       "       [-0.09402926, -0.04471768],\n",
       "       [-0.09282194, -0.04423779],\n",
       "       [-0.09357215, -0.04429042],\n",
       "       [-0.09357224, -0.04429041],\n",
       "       [-0.093572  , -0.04429051],\n",
       "       [-0.09387506, -0.04423635],\n",
       "       [-0.09357217, -0.04429039],\n",
       "       [-0.09357226, -0.04429047],\n",
       "       [-0.09357231, -0.04429047],\n",
       "       [-0.09357213, -0.04429042],\n",
       "       [-0.09402931, -0.04471763],\n",
       "       [-0.0935721 , -0.04429045],\n",
       "       [-0.0935723 , -0.04429037],\n",
       "       [-0.09390495, -0.04423497],\n",
       "       [-0.09286226, -0.04423801],\n",
       "       [-0.09357225, -0.04429044],\n",
       "       [-0.09292118, -0.04423841],\n",
       "       [-0.09402922, -0.04471767],\n",
       "       [-0.09438466, -0.04555735],\n",
       "       [-0.09352824, -0.04567604],\n",
       "       [-0.09357213, -0.04429041],\n",
       "       [-0.09438466, -0.04555738],\n",
       "       [ 0.5658342 , -0.7516972 ],\n",
       "       [-0.68195915,  0.9281123 ],\n",
       "       [-0.16804343,  0.8969723 ],\n",
       "       [-0.21561533,  0.8872479 ],\n",
       "       [ 0.33311784,  1.4848857 ],\n",
       "       [-0.49146646, -0.9112908 ],\n",
       "       [-0.7840006 ,  0.64221   ],\n",
       "       [-1.6870674 , -0.01502673],\n",
       "       [-1.683569  , -0.31268868],\n",
       "       [ 0.15192376,  1.5169169 ],\n",
       "       [ 0.5519774 , -0.7553696 ],\n",
       "       [-0.19345444,  0.89312434],\n",
       "       [ 0.00954439, -0.9852904 ],\n",
       "       [ 1.2529213 , -0.86898863],\n",
       "       [ 0.7157559 , -1.4068545 ],\n",
       "       [-1.0105442 ,  0.0666644 ],\n",
       "       [-0.49143532, -0.91130733],\n",
       "       [-1.6781327 , -0.5238386 ],\n",
       "       [ 0.7070064 , -1.3532443 ],\n",
       "       [-1.0912485 ,  0.2609709 ],\n",
       "       [ 0.5914486 ,  0.6030854 ],\n",
       "       [ 0.29007277,  0.85752374],\n",
       "       [-0.78400457,  0.64221054],\n",
       "       [ 0.82696617,  0.05020912],\n",
       "       [-0.9157933 ,  0.39666325],\n",
       "       [ 0.48047927,  0.6824302 ],\n",
       "       [ 0.699122  ,  1.3433238 ],\n",
       "       [-1.527756  ,  0.7711177 ],\n",
       "       [-0.17648299, -0.9404305 ],\n",
       "       [ 0.00489504, -1.6182714 ],\n",
       "       [-0.745333  , -0.75456744],\n",
       "       [ 0.07446985, -1.6098263 ],\n",
       "       [-0.90921056, -0.51909876],\n",
       "       [-0.35309744, -1.6927862 ],\n",
       "       [-0.7549167 , -0.7480772 ],\n",
       "       [-1.5303789 ,  0.8252883 ],\n",
       "       [-0.64414907,  0.9161308 ],\n",
       "       [-0.40908116,  1.5942616 ],\n",
       "       [ 0.77552116, -0.45742977],\n",
       "       [-1.7139313 ,  0.34689873],\n",
       "       [-1.2000505 , -1.3149167 ],\n",
       "       [ 0.56672233,  0.62378716],\n",
       "       [-1.6966333 , -0.25881213],\n",
       "       [-1.2592423 , -1.3518304 ],\n",
       "       [-0.60986096, -1.6028228 ],\n",
       "       [ 0.5571199 , -0.7537022 ],\n",
       "       [-0.18149003, -0.9404475 ],\n",
       "       [ 0.652052  ,  0.5439152 ],\n",
       "       [-1.0135096 ,  0.07994334],\n",
       "       [-1.4026251 ,  0.99942905],\n",
       "       [ 0.8034564 ,  0.2814297 ],\n",
       "       [-1.0139787 , -0.38622704],\n",
       "       [-0.90936255,  0.41910285],\n",
       "       [-1.7069834 , -0.21646161],\n",
       "       [-1.2487442 ,  1.2091386 ],\n",
       "       [ 0.70700836, -1.3532406 ],\n",
       "       [ 0.775364  , -0.43251252],\n",
       "       [ 1.4593084 ,  0.5013817 ],\n",
       "       [-0.9091942 , -0.5191297 ],\n",
       "       [-0.7542853 ,  0.64790285],\n",
       "       [-0.6330288 ,  1.5276138 ],\n",
       "       [-0.4914671 , -0.9112894 ],\n",
       "       [ 0.81515795,  0.27251384],\n",
       "       [ 0.8151549 ,  0.2725126 ],\n",
       "       [-0.48146433, -0.9555993 ],\n",
       "       [ 0.23406453, -0.96738976],\n",
       "       [-0.09453246, -0.04422918],\n",
       "       [-0.75614524, -0.7471462 ],\n",
       "       [-0.33923775, -1.7070334 ],\n",
       "       [-0.04997944,  0.8653518 ],\n",
       "       [-0.25361592,  0.92077893],\n",
       "       [ 1.4627981 , -0.31868872],\n",
       "       [ 0.01109826, -0.9919072 ],\n",
       "       [ 0.8378772 , -0.4608755 ],\n",
       "       [ 1.5167757 ,  0.09222056],\n",
       "       [-0.90735453,  0.48416185],\n",
       "       [-0.6891969 ,  0.6840785 ],\n",
       "       [ 1.4683124 , -0.39760384],\n",
       "       [-0.91579276,  0.39665794],\n",
       "       [-1.4517329 , -0.8349483 ],\n",
       "       [ 1.4556825 , -0.43950722],\n",
       "       [-0.90385836, -0.53746456],\n",
       "       [-1.6818422 ,  0.39974082],\n",
       "       [ 0.3794734 , -1.5882617 ],\n",
       "       [-0.7149603 ,  0.6763423 ],\n",
       "       [-1.5010135 , -0.8125622 ],\n",
       "       [ 1.3821416 ,  0.6221542 ],\n",
       "       [-1.0105407 ,  0.06667065],\n",
       "       [ 0.5677141 , -0.75346583],\n",
       "       [-0.7510912 ,  0.93569535],\n",
       "       [-1.6733938 ,  0.424968  ],\n",
       "       [-0.10111515,  1.5582751 ],\n",
       "       [ 0.34608585,  1.5217464 ],\n",
       "       [-0.18446156, -1.6186281 ],\n",
       "       [ 0.20608877, -0.9794269 ],\n",
       "       [ 1.2417933 , -1.0248709 ],\n",
       "       [-1.1023492 ,  0.22869584],\n",
       "       [ 0.5524828 , -0.75502235],\n",
       "       [ 0.9482207 , -1.2957047 ],\n",
       "       [-0.7453346 , -0.7545686 ],\n",
       "       [ 0.02969779,  1.5965616 ],\n",
       "       [-0.65084684, -1.5277505 ],\n",
       "       [-1.6871145 , -0.01502249],\n",
       "       [ 1.4556793 , -0.43950653],\n",
       "       [-0.6886534 ,  0.94051594],\n",
       "       [-0.7149561 ,  0.67633075],\n",
       "       [ 0.82890224, -0.4376091 ],\n",
       "       [ 1.2500063 , -1.0179305 ],\n",
       "       [-1.1880509 ,  1.203196  ],\n",
       "       [ 0.92807204, -1.3564168 ],\n",
       "       [ 0.40682814,  0.70839596],\n",
       "       [-0.09081651, -0.94080526],\n",
       "       [ 0.5519719 , -0.7553766 ],\n",
       "       [-1.0139802 , -0.38621452],\n",
       "       [ 0.59144396,  0.6030859 ],\n",
       "       [-0.66980094, -1.4557233 ],\n",
       "       [-1.0254374 ,  0.05934078],\n",
       "       [ 0.10408168,  0.89671314],\n",
       "       [-0.6819094 ,  0.9280765 ],\n",
       "       [ 0.82748723,  0.04437715],\n",
       "       [-1.5291823 , -0.7812291 ],\n",
       "       [-0.48457217, -0.9037697 ],\n",
       "       [ 0.5683589 , -0.7523874 ],\n",
       "       [ 0.02841788, -0.97448164],\n",
       "       [ 0.29632136, -0.86999595],\n",
       "       [ 0.38475367,  0.74820876],\n",
       "       [ 0.05720037, -0.9821805 ],\n",
       "       [ 0.88472265, -0.29383308],\n",
       "       [ 1.5002608 ,  0.11783522],\n",
       "       [-0.7338365 , -1.5769951 ],\n",
       "       [-0.63302946,  1.5276163 ],\n",
       "       [-0.39081845,  1.5902431 ],\n",
       "       [ 0.8414023 ,  0.2294485 ],\n",
       "       [ 1.0646276 ,  1.1613402 ],\n",
       "       [-1.2382166 , -1.3553655 ],\n",
       "       [-0.25361434,  0.9207803 ],\n",
       "       [ 0.83292603, -0.45170537],\n",
       "       [-1.0043999 , -0.3965583 ],\n",
       "       [-0.14282435, -1.568425  ],\n",
       "       [-0.90924823, -0.5190351 ],\n",
       "       [ 0.3163674 ,  0.83530825],\n",
       "       [ 0.5407093 ,  0.64038676],\n",
       "       [ 0.54732233, -1.4675915 ],\n",
       "       [-1.554961  , -0.7834824 ],\n",
       "       [-1.0042384 , -0.3970295 ],\n",
       "       [ 0.747946  ,  0.37826818],\n",
       "       [ 1.2555438 ,  1.0496976 ],\n",
       "       [ 1.508633  ,  0.08601352],\n",
       "       [ 0.8958215 ,  1.2371058 ],\n",
       "       [ 0.81803554, -0.0332973 ],\n",
       "       [-1.2716669 ,  1.1963673 ],\n",
       "       [ 0.39322513,  0.75511986],\n",
       "       [ 0.88609904,  1.2578967 ],\n",
       "       [ 0.8251007 ,  0.04574275],\n",
       "       [-0.48146728, -0.9555989 ],\n",
       "       [ 0.03891575, -1.6105243 ],\n",
       "       [-1.683567  , -0.31268638],\n",
       "       [-1.0042323 , -0.39704892],\n",
       "       [-0.54621524, -0.8589609 ],\n",
       "       [-1.0105402 ,  0.06667075],\n",
       "       [ 0.82748747,  0.0443786 ],\n",
       "       [-1.1023483 ,  0.2286936 ],\n",
       "       [-1.0443926 , -0.2186119 ],\n",
       "       [ 0.83833385, -0.45908555],\n",
       "       [-0.49142742, -0.91130656],\n",
       "       [ 0.35408223,  1.4984806 ],\n",
       "       [ 1.4191227 ,  0.48759413],\n",
       "       [-1.7182165 ,  0.33716458],\n",
       "       [-0.04997522,  0.8653591 ],\n",
       "       [ 0.7758013 , -0.4492181 ],\n",
       "       [-1.0254365 ,  0.05934649],\n",
       "       [ 0.5671185 ,  1.4610809 ],\n",
       "       [-1.6941825 ,  0.39834526],\n",
       "       [-0.04867414,  0.8729707 ],\n",
       "       [ 1.0761803 ,  1.0138434 ],\n",
       "       [ 1.4999951 , -0.18613324],\n",
       "       [ 0.36616802, -0.8339545 ],\n",
       "       [ 1.3043402 ,  0.78751916],\n",
       "       [ 0.8012574 ,  0.30999887],\n",
       "       [-0.17841521,  0.888071  ],\n",
       "       [ 1.2250192 , -0.9246576 ],\n",
       "       [-1.058272  , -0.18129557],\n",
       "       [-0.6330294 ,  1.5276154 ],\n",
       "       [ 0.35126874, -1.6574794 ],\n",
       "       [ 1.5361544 ,  0.00651319],\n",
       "       [ 0.43356428, -1.5421095 ],\n",
       "       [ 0.02841759, -0.97448146],\n",
       "       [-0.7783153 ,  0.6298388 ],\n",
       "       [ 0.82756144,  0.04425807],\n",
       "       [ 0.29631522, -0.8699921 ],\n",
       "       [ 1.0178202 ,  1.086877  ],\n",
       "       [-0.90321296,  1.4314071 ],\n",
       "       [-1.0736407 , -1.2938344 ],\n",
       "       [ 0.5914392 ,  0.6030922 ],\n",
       "       [-1.2570758 ,  1.1891763 ],\n",
       "       [-1.0139259 ,  0.07798926],\n",
       "       [ 1.225848  , -0.9382373 ],\n",
       "       [-0.48361093, -0.96345216],\n",
       "       [-0.9091989 , -0.51912   ],\n",
       "       [-1.2382178 , -1.3553619 ],\n",
       "       [-1.6489271 , -0.7986506 ],\n",
       "       [ 1.3054343 ,  0.784621  ],\n",
       "       [-0.83416563,  1.4812069 ],\n",
       "       [-0.21561387,  0.8872475 ],\n",
       "       [ 0.6486003 ,  1.4263295 ],\n",
       "       [-0.17573144,  1.5861366 ],\n",
       "       [-1.5281833 ,  0.7139844 ],\n",
       "       [ 0.04546732,  0.8982583 ],\n",
       "       [-0.74533516, -0.75457096],\n",
       "       [ 0.77308077, -0.47203696],\n",
       "       [ 0.4237445 ,  0.7158467 ],\n",
       "       [ 0.22280511, -0.90221024],\n",
       "       [-0.10364742,  1.5920609 ],\n",
       "       [ 0.5558983 ,  1.4016231 ],\n",
       "       [ 1.3168318 ,  0.82423353],\n",
       "       [-0.472046  , -1.6403676 ],\n",
       "       [ 1.3985395 , -0.5430933 ],\n",
       "       [ 0.82569015,  0.18818015],\n",
       "       [ 1.3288274 , -0.6816253 ],\n",
       "       [-1.238155  , -1.1993921 ],\n",
       "       [-0.7783183 ,  0.62983227],\n",
       "       [ 0.04546754,  0.89826065],\n",
       "       [ 1.4949061 ,  0.4368289 ],\n",
       "       [ 0.7176785 , -1.3884465 ],\n",
       "       [ 0.83727914, -0.45964974],\n",
       "       [-0.09281017, -0.04423764]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tsne_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  6.  2.  0.  0.  4.  0.  2.  4.  0.  0.  0.  0.  0. 26.  4.  4.  4.\n",
      "  0.  0.  0.  4.  0.  0.  0.  0.  7.  0.  3.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  5.,  4.,  0.,  0.,  5.,  0.,  3.,  3.,  0.,  0.,  0.,  0.,\n",
       "        0., 20.,  5.,  4.,  7.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,\n",
       "        5.,  0.,  6.,  0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(x_train[0]))\n",
    "sum(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape =  Tensor(\"merged_layer/concat:0\", shape=(?, 9), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 70, 30, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 70, 30, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 70, 30, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 3)            42463       anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 9)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 42,463\n",
      "Trainable params: 42,423\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    print('y_pred.shape = ',y_pred)\n",
    "    \n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "    \n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss\n",
    "\n",
    "def baseNetwork():    \n",
    "        model = Sequential()\n",
    "        model.add(Reshape((70, 30), input_shape=(1004,None, None)))\n",
    "        model.add(Conv1D(20,10,activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D(20,5,activation='relu'))\n",
    "        model.add(Conv1D(20,3,activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        return model\n",
    "    \n",
    "anchor_input = Input((70,30,1, ), name='anchor_input')\n",
    "positive_input = Input((70,30,1, ), name='positive_input')\n",
    "negative_input = Input((70,30,1, ), name='negative_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "Shared_DNN = baseNetwork()\n",
    "\n",
    "\n",
    "encoded_anchor = Shared_DNN(anchor_input)\n",
    "encoded_positive = Shared_DNN(positive_input)\n",
    "encoded_negative = Shared_DNN(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "model.compile(loss=triplet_loss, optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateTriplet(x,y,testsize=0.2,ap_pairs=10,an_pairs=10):\n",
    "    data_xy = tuple([x,y])\n",
    "\n",
    "    trainsize = 1-testsize\n",
    "\n",
    "    triplet_train_pairs = []\n",
    "    triplet_test_pairs = []\n",
    "    for data_class in sorted(set(data_xy[1])):\n",
    "\n",
    "        same_class_idx = np.where((data_xy[1] == data_class))[0]\n",
    "        diff_class_idx = np.where(data_xy[1] != data_class)[0]\n",
    "        A_P_pairs = random.sample(list(permutations(same_class_idx,2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
    "        Neg_idx = random.sample(list(diff_class_idx),k=an_pairs)\n",
    "        \n",
    "\n",
    "        #train\n",
    "        A_P_len = len(A_P_pairs)\n",
    "        Neg_len = len(Neg_idx)\n",
    "        for ap in A_P_pairs[:int(A_P_len*trainsize)]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_train_pairs.append([Anchor,Positive,Negative])               \n",
    "        #test\n",
    "        for ap in A_P_pairs[int(A_P_len*trainsize):]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_test_pairs.append([Anchor,Positive,Negative])    \n",
    "                \n",
    "    return np.array(triplet_train_pairs), np.array(triplet_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = generateTriplet(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 60 samples\n",
      "Epoch 1/50\n",
      "240/240 [==============================] - 2s 8ms/step - loss: 0.3703 - val_loss: 0.3210\n",
      "Epoch 2/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.2789 - val_loss: 0.2584\n",
      "Epoch 3/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.2076 - val_loss: 0.2463\n",
      "Epoch 4/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.1737 - val_loss: 0.2960\n",
      "Epoch 5/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.1230 - val_loss: 0.3644\n",
      "Epoch 6/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.1054 - val_loss: 0.4253\n",
      "Epoch 7/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.1107 - val_loss: 0.4260\n",
      "Epoch 8/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.1060 - val_loss: 0.3907\n",
      "Epoch 9/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0765 - val_loss: 0.4325\n",
      "Epoch 10/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0650 - val_loss: 0.4884\n",
      "Epoch 11/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0723 - val_loss: 0.5402\n",
      "Epoch 12/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0532 - val_loss: 0.6048\n",
      "Epoch 13/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0468 - val_loss: 0.5988\n",
      "Epoch 14/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0384 - val_loss: 0.6010\n",
      "Epoch 15/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0377 - val_loss: 0.6161\n",
      "Epoch 16/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 0.6284\n",
      "Epoch 17/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0276 - val_loss: 0.6845\n",
      "Epoch 18/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 0.7190\n",
      "Epoch 19/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.6843\n",
      "Epoch 20/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0292 - val_loss: 0.6473\n",
      "Epoch 21/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0223 - val_loss: 0.6390\n",
      "Epoch 22/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.6578\n",
      "Epoch 23/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.6656\n",
      "Epoch 24/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.6921\n",
      "Epoch 25/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.7157\n",
      "Epoch 26/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0186 - val_loss: 0.7450\n",
      "Epoch 27/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.7511\n",
      "Epoch 28/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.7398\n",
      "Epoch 29/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.7284\n",
      "Epoch 30/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0176 - val_loss: 0.7363\n",
      "Epoch 31/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0081 - val_loss: 0.7525\n",
      "Epoch 32/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.7354\n",
      "Epoch 33/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0104 - val_loss: 0.7318\n",
      "Epoch 34/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.7247\n",
      "Epoch 35/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0113 - val_loss: 0.7400\n",
      "Epoch 36/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.7057\n",
      "Epoch 37/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0115 - val_loss: 0.6817\n",
      "Epoch 38/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0089 - val_loss: 0.6687\n",
      "Epoch 39/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0087 - val_loss: 0.6359\n",
      "Epoch 40/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0089 - val_loss: 0.6559\n",
      "Epoch 41/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0101 - val_loss: 0.6666\n",
      "Epoch 42/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.6795\n",
      "Epoch 43/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0088 - val_loss: 0.6940\n",
      "Epoch 44/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0083 - val_loss: 0.6943\n",
      "Epoch 45/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.6854\n",
      "Epoch 46/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.6729\n",
      "Epoch 47/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0017 - val_loss: 0.6459\n",
      "Epoch 48/50\n",
      "240/240 [==============================] - 0s 999us/step - loss: 0.0048 - val_loss: 0.6470\n",
      "Epoch 49/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.6570\n",
      "Epoch 50/50\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.6584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a27c9da20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Anchor = X_train[:,0,:].reshape(-1,70,30,1)\n",
    "Positive = X_train[:,1,:].reshape(-1,70,30,1)\n",
    "Negative = X_train[:,2,:].reshape(-1,70,30,1)\n",
    "Anchor_test = X_test[:,0,:].reshape(-1,70,30,1)\n",
    "Positive_test = X_test[:,1,:].reshape(-1,70,30,1)\n",
    "Negative_test = X_test[:,2,:].reshape(-1,70,30,1)\n",
    "\n",
    "Y_dummy = np.empty((Anchor.shape[0],300))\n",
    "Y_dummy2 = np.empty((Anchor_test.shape[0],1))\n",
    "\n",
    "model.fit([Anchor,Positive,Negative],y=Y_dummy,validation_data=([Anchor_test,Positive_test,Negative_test],Y_dummy2), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trained_model = Model(inputs=anchor_input, outputs=encoded_anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/neighbors/base.py:316: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = np.sqrt(dist[sample_range, neigh_ind]), neigh_ind\n"
     ]
    }
   ],
   "source": [
    "tsne = TSNE()\n",
    "X_train_trm = trained_model.predict(x_train.reshape(-1,70,30,1))\n",
    "X_test_trm = trained_model.predict(x_test.reshape(-1,70,30,1))\n",
    "train_tsne_embeds = tsne.fit_transform(X_train_trm)\n",
    "eval_tsne_embeds = tsne.fit_transform(X_test_trm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    }
   ],
   "source": [
    "scatter(train_tsne_embeds, y_train, \"Training Data SMILES After TNN\")\n",
    "scatter(eval_tsne_embeds, y_test, \"Validation Data After TNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 316 samples, validate on 106 samples\n",
      "Epoch 1/100\n",
      "316/316 [==============================] - 0s 1ms/step - loss: 1.0975 - acc: 0.3766 - val_loss: 1.0979 - val_acc: 0.3679\n",
      "Epoch 2/100\n",
      "316/316 [==============================] - 0s 74us/step - loss: 1.0950 - acc: 0.3766 - val_loss: 1.0947 - val_acc: 0.3679\n",
      "Epoch 3/100\n",
      "316/316 [==============================] - 0s 92us/step - loss: 1.0926 - acc: 0.3766 - val_loss: 1.0923 - val_acc: 0.3774\n",
      "Epoch 4/100\n",
      "316/316 [==============================] - 0s 90us/step - loss: 1.0908 - acc: 0.3766 - val_loss: 1.0898 - val_acc: 0.3774\n",
      "Epoch 5/100\n",
      "316/316 [==============================] - 0s 68us/step - loss: 1.0893 - acc: 0.3797 - val_loss: 1.0872 - val_acc: 0.3774\n",
      "Epoch 6/100\n",
      "316/316 [==============================] - 0s 90us/step - loss: 1.0873 - acc: 0.3797 - val_loss: 1.0851 - val_acc: 0.3679\n",
      "Epoch 7/100\n",
      "316/316 [==============================] - 0s 80us/step - loss: 1.0859 - acc: 0.3797 - val_loss: 1.0830 - val_acc: 0.3679\n",
      "Epoch 8/100\n",
      "316/316 [==============================] - 0s 92us/step - loss: 1.0844 - acc: 0.3766 - val_loss: 1.0812 - val_acc: 0.3679\n",
      "Epoch 9/100\n",
      "316/316 [==============================] - 0s 96us/step - loss: 1.0830 - acc: 0.3766 - val_loss: 1.0795 - val_acc: 0.3679\n",
      "Epoch 10/100\n",
      "316/316 [==============================] - 0s 74us/step - loss: 1.0819 - acc: 0.3766 - val_loss: 1.0777 - val_acc: 0.3679\n",
      "Epoch 11/100\n",
      "316/316 [==============================] - 0s 80us/step - loss: 1.0808 - acc: 0.3766 - val_loss: 1.0760 - val_acc: 0.3679\n",
      "Epoch 12/100\n",
      "316/316 [==============================] - 0s 75us/step - loss: 1.0795 - acc: 0.3766 - val_loss: 1.0746 - val_acc: 0.3679\n",
      "Epoch 13/100\n",
      "316/316 [==============================] - 0s 79us/step - loss: 1.0784 - acc: 0.3797 - val_loss: 1.0733 - val_acc: 0.3679\n",
      "Epoch 14/100\n",
      "316/316 [==============================] - 0s 70us/step - loss: 1.0775 - acc: 0.3797 - val_loss: 1.0720 - val_acc: 0.3679\n",
      "Epoch 15/100\n",
      "316/316 [==============================] - 0s 87us/step - loss: 1.0765 - acc: 0.3797 - val_loss: 1.0709 - val_acc: 0.3774\n",
      "Epoch 16/100\n",
      "316/316 [==============================] - 0s 55us/step - loss: 1.0757 - acc: 0.3861 - val_loss: 1.0696 - val_acc: 0.3679\n",
      "Epoch 17/100\n",
      "316/316 [==============================] - 0s 59us/step - loss: 1.0748 - acc: 0.4051 - val_loss: 1.0684 - val_acc: 0.3774\n",
      "Epoch 18/100\n",
      "316/316 [==============================] - 0s 86us/step - loss: 1.0739 - acc: 0.4430 - val_loss: 1.0675 - val_acc: 0.3868\n",
      "Epoch 19/100\n",
      "316/316 [==============================] - 0s 82us/step - loss: 1.0732 - acc: 0.4557 - val_loss: 1.0665 - val_acc: 0.4151\n",
      "Epoch 20/100\n",
      "316/316 [==============================] - 0s 133us/step - loss: 1.0725 - acc: 0.4589 - val_loss: 1.0656 - val_acc: 0.4151\n",
      "Epoch 21/100\n",
      "316/316 [==============================] - 0s 159us/step - loss: 1.0718 - acc: 0.4684 - val_loss: 1.0646 - val_acc: 0.4623\n",
      "Epoch 22/100\n",
      "316/316 [==============================] - 0s 139us/step - loss: 1.0711 - acc: 0.4747 - val_loss: 1.0640 - val_acc: 0.4623\n",
      "Epoch 23/100\n",
      "316/316 [==============================] - 0s 149us/step - loss: 1.0704 - acc: 0.4652 - val_loss: 1.0630 - val_acc: 0.4528\n",
      "Epoch 24/100\n",
      "316/316 [==============================] - 0s 155us/step - loss: 1.0698 - acc: 0.4684 - val_loss: 1.0623 - val_acc: 0.4623\n",
      "Epoch 25/100\n",
      "316/316 [==============================] - 0s 109us/step - loss: 1.0692 - acc: 0.4684 - val_loss: 1.0615 - val_acc: 0.4811\n",
      "Epoch 26/100\n",
      "316/316 [==============================] - 0s 102us/step - loss: 1.0686 - acc: 0.4684 - val_loss: 1.0609 - val_acc: 0.4811\n",
      "Epoch 27/100\n",
      "316/316 [==============================] - 0s 70us/step - loss: 1.0681 - acc: 0.4684 - val_loss: 1.0603 - val_acc: 0.4811\n",
      "Epoch 28/100\n",
      "316/316 [==============================] - 0s 85us/step - loss: 1.0675 - acc: 0.4747 - val_loss: 1.0595 - val_acc: 0.4811\n",
      "Epoch 29/100\n",
      "316/316 [==============================] - 0s 52us/step - loss: 1.0669 - acc: 0.4747 - val_loss: 1.0588 - val_acc: 0.4811\n",
      "Epoch 30/100\n",
      "316/316 [==============================] - 0s 73us/step - loss: 1.0664 - acc: 0.4747 - val_loss: 1.0583 - val_acc: 0.4906\n",
      "Epoch 31/100\n",
      "316/316 [==============================] - 0s 67us/step - loss: 1.0660 - acc: 0.4810 - val_loss: 1.0576 - val_acc: 0.4906\n",
      "Epoch 32/100\n",
      "316/316 [==============================] - 0s 95us/step - loss: 1.0654 - acc: 0.4810 - val_loss: 1.0572 - val_acc: 0.4906\n",
      "Epoch 33/100\n",
      "316/316 [==============================] - 0s 84us/step - loss: 1.0650 - acc: 0.4715 - val_loss: 1.0566 - val_acc: 0.4906\n",
      "Epoch 34/100\n",
      "316/316 [==============================] - 0s 61us/step - loss: 1.0645 - acc: 0.4747 - val_loss: 1.0562 - val_acc: 0.4906\n",
      "Epoch 35/100\n",
      "316/316 [==============================] - 0s 81us/step - loss: 1.0641 - acc: 0.4747 - val_loss: 1.0557 - val_acc: 0.4906\n",
      "Epoch 36/100\n",
      "316/316 [==============================] - 0s 74us/step - loss: 1.0636 - acc: 0.4778 - val_loss: 1.0552 - val_acc: 0.4906\n",
      "Epoch 37/100\n",
      "316/316 [==============================] - 0s 147us/step - loss: 1.0633 - acc: 0.4747 - val_loss: 1.0549 - val_acc: 0.4906\n",
      "Epoch 38/100\n",
      "316/316 [==============================] - 0s 78us/step - loss: 1.0628 - acc: 0.4747 - val_loss: 1.0543 - val_acc: 0.4906\n",
      "Epoch 39/100\n",
      "316/316 [==============================] - 0s 92us/step - loss: 1.0624 - acc: 0.4747 - val_loss: 1.0538 - val_acc: 0.5000\n",
      "Epoch 40/100\n",
      "316/316 [==============================] - 0s 90us/step - loss: 1.0620 - acc: 0.4747 - val_loss: 1.0535 - val_acc: 0.5000\n",
      "Epoch 41/100\n",
      "316/316 [==============================] - 0s 89us/step - loss: 1.0617 - acc: 0.4747 - val_loss: 1.0530 - val_acc: 0.4906\n",
      "Epoch 42/100\n",
      "316/316 [==============================] - 0s 84us/step - loss: 1.0612 - acc: 0.4715 - val_loss: 1.0526 - val_acc: 0.4906\n",
      "Epoch 43/100\n",
      "316/316 [==============================] - 0s 283us/step - loss: 1.0609 - acc: 0.4715 - val_loss: 1.0522 - val_acc: 0.4906\n",
      "Epoch 44/100\n",
      "316/316 [==============================] - 0s 496us/step - loss: 1.0605 - acc: 0.4715 - val_loss: 1.0519 - val_acc: 0.4906\n",
      "Epoch 45/100\n",
      "316/316 [==============================] - 0s 82us/step - loss: 1.0602 - acc: 0.4747 - val_loss: 1.0515 - val_acc: 0.4906\n",
      "Epoch 46/100\n",
      "316/316 [==============================] - 0s 128us/step - loss: 1.0599 - acc: 0.4715 - val_loss: 1.0514 - val_acc: 0.4906\n",
      "Epoch 47/100\n",
      "316/316 [==============================] - 0s 111us/step - loss: 1.0596 - acc: 0.4747 - val_loss: 1.0509 - val_acc: 0.4906\n",
      "Epoch 48/100\n",
      "316/316 [==============================] - 0s 65us/step - loss: 1.0593 - acc: 0.4747 - val_loss: 1.0506 - val_acc: 0.4906\n",
      "Epoch 49/100\n",
      "316/316 [==============================] - 0s 183us/step - loss: 1.0588 - acc: 0.4747 - val_loss: 1.0504 - val_acc: 0.4906\n",
      "Epoch 50/100\n",
      "316/316 [==============================] - 0s 96us/step - loss: 1.0586 - acc: 0.4747 - val_loss: 1.0502 - val_acc: 0.4906\n",
      "Epoch 51/100\n",
      "316/316 [==============================] - 0s 115us/step - loss: 1.0583 - acc: 0.4747 - val_loss: 1.0500 - val_acc: 0.4906\n",
      "Epoch 52/100\n",
      "316/316 [==============================] - 0s 94us/step - loss: 1.0580 - acc: 0.4715 - val_loss: 1.0496 - val_acc: 0.4906\n",
      "Epoch 53/100\n",
      "316/316 [==============================] - 0s 102us/step - loss: 1.0578 - acc: 0.4747 - val_loss: 1.0495 - val_acc: 0.4906\n",
      "Epoch 54/100\n",
      "316/316 [==============================] - 0s 63us/step - loss: 1.0576 - acc: 0.4715 - val_loss: 1.0490 - val_acc: 0.4906\n",
      "Epoch 55/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 1.0571 - acc: 0.4715 - val_loss: 1.0488 - val_acc: 0.4906\n",
      "Epoch 56/100\n",
      "316/316 [==============================] - 0s 97us/step - loss: 1.0570 - acc: 0.4715 - val_loss: 1.0487 - val_acc: 0.4906\n",
      "Epoch 57/100\n",
      "316/316 [==============================] - 0s 125us/step - loss: 1.0566 - acc: 0.4715 - val_loss: 1.0485 - val_acc: 0.4906\n",
      "Epoch 58/100\n",
      "316/316 [==============================] - 0s 93us/step - loss: 1.0564 - acc: 0.4684 - val_loss: 1.0482 - val_acc: 0.4906\n",
      "Epoch 59/100\n",
      "316/316 [==============================] - 0s 72us/step - loss: 1.0562 - acc: 0.4684 - val_loss: 1.0480 - val_acc: 0.4906\n",
      "Epoch 60/100\n",
      "316/316 [==============================] - 0s 137us/step - loss: 1.0559 - acc: 0.4684 - val_loss: 1.0477 - val_acc: 0.4906\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s 131us/step - loss: 1.0556 - acc: 0.4684 - val_loss: 1.0476 - val_acc: 0.4906\n",
      "Epoch 62/100\n",
      "316/316 [==============================] - 0s 71us/step - loss: 1.0554 - acc: 0.4684 - val_loss: 1.0473 - val_acc: 0.4906\n",
      "Epoch 63/100\n",
      "316/316 [==============================] - 0s 80us/step - loss: 1.0552 - acc: 0.4684 - val_loss: 1.0472 - val_acc: 0.4906\n",
      "Epoch 64/100\n",
      "316/316 [==============================] - 0s 80us/step - loss: 1.0549 - acc: 0.4684 - val_loss: 1.0471 - val_acc: 0.4906\n",
      "Epoch 65/100\n",
      "316/316 [==============================] - 0s 76us/step - loss: 1.0549 - acc: 0.4684 - val_loss: 1.0468 - val_acc: 0.4906\n",
      "Epoch 66/100\n",
      "316/316 [==============================] - 0s 75us/step - loss: 1.0545 - acc: 0.4684 - val_loss: 1.0467 - val_acc: 0.4906\n",
      "Epoch 67/100\n",
      "316/316 [==============================] - 0s 59us/step - loss: 1.0543 - acc: 0.4684 - val_loss: 1.0466 - val_acc: 0.4906\n",
      "Epoch 68/100\n",
      "316/316 [==============================] - 0s 77us/step - loss: 1.0541 - acc: 0.4684 - val_loss: 1.0464 - val_acc: 0.4906\n",
      "Epoch 69/100\n",
      "316/316 [==============================] - 0s 67us/step - loss: 1.0539 - acc: 0.4684 - val_loss: 1.0462 - val_acc: 0.4906\n",
      "Epoch 70/100\n",
      "316/316 [==============================] - 0s 67us/step - loss: 1.0537 - acc: 0.4684 - val_loss: 1.0461 - val_acc: 0.4906\n",
      "Epoch 71/100\n",
      "316/316 [==============================] - 0s 66us/step - loss: 1.0535 - acc: 0.4684 - val_loss: 1.0459 - val_acc: 0.4906\n",
      "Epoch 72/100\n",
      "316/316 [==============================] - 0s 76us/step - loss: 1.0533 - acc: 0.4684 - val_loss: 1.0458 - val_acc: 0.4906\n",
      "Epoch 73/100\n",
      "316/316 [==============================] - 0s 82us/step - loss: 1.0531 - acc: 0.4684 - val_loss: 1.0457 - val_acc: 0.4906\n",
      "Epoch 74/100\n",
      "316/316 [==============================] - 0s 58us/step - loss: 1.0530 - acc: 0.4684 - val_loss: 1.0455 - val_acc: 0.4906\n",
      "Epoch 75/100\n",
      "316/316 [==============================] - 0s 80us/step - loss: 1.0529 - acc: 0.4684 - val_loss: 1.0455 - val_acc: 0.4906\n",
      "Epoch 76/100\n",
      "316/316 [==============================] - 0s 55us/step - loss: 1.0527 - acc: 0.4684 - val_loss: 1.0453 - val_acc: 0.4906\n",
      "Epoch 77/100\n",
      "316/316 [==============================] - 0s 64us/step - loss: 1.0525 - acc: 0.4684 - val_loss: 1.0453 - val_acc: 0.4906\n",
      "Epoch 78/100\n",
      "316/316 [==============================] - 0s 69us/step - loss: 1.0523 - acc: 0.4684 - val_loss: 1.0451 - val_acc: 0.4906\n",
      "Epoch 79/100\n",
      "316/316 [==============================] - 0s 48us/step - loss: 1.0521 - acc: 0.4684 - val_loss: 1.0450 - val_acc: 0.4906\n",
      "Epoch 80/100\n",
      "316/316 [==============================] - 0s 65us/step - loss: 1.0520 - acc: 0.4715 - val_loss: 1.0448 - val_acc: 0.4906\n",
      "Epoch 81/100\n",
      "316/316 [==============================] - 0s 124us/step - loss: 1.0519 - acc: 0.4715 - val_loss: 1.0447 - val_acc: 0.4906\n",
      "Epoch 82/100\n",
      "316/316 [==============================] - 0s 157us/step - loss: 1.0517 - acc: 0.4715 - val_loss: 1.0446 - val_acc: 0.4906\n",
      "Epoch 83/100\n",
      "316/316 [==============================] - 0s 148us/step - loss: 1.0516 - acc: 0.4715 - val_loss: 1.0446 - val_acc: 0.4906\n",
      "Epoch 84/100\n",
      "316/316 [==============================] - 0s 106us/step - loss: 1.0514 - acc: 0.4715 - val_loss: 1.0444 - val_acc: 0.4906\n",
      "Epoch 85/100\n",
      "316/316 [==============================] - 0s 135us/step - loss: 1.0513 - acc: 0.4715 - val_loss: 1.0442 - val_acc: 0.4906\n",
      "Epoch 86/100\n",
      "316/316 [==============================] - 0s 91us/step - loss: 1.0511 - acc: 0.4715 - val_loss: 1.0441 - val_acc: 0.4906\n",
      "Epoch 87/100\n",
      "316/316 [==============================] - 0s 406us/step - loss: 1.0510 - acc: 0.4715 - val_loss: 1.0442 - val_acc: 0.4906\n",
      "Epoch 88/100\n",
      "316/316 [==============================] - 0s 98us/step - loss: 1.0509 - acc: 0.4715 - val_loss: 1.0441 - val_acc: 0.4906\n",
      "Epoch 89/100\n",
      "316/316 [==============================] - 0s 100us/step - loss: 1.0507 - acc: 0.4715 - val_loss: 1.0440 - val_acc: 0.4906\n",
      "Epoch 90/100\n",
      "316/316 [==============================] - 0s 82us/step - loss: 1.0506 - acc: 0.4715 - val_loss: 1.0439 - val_acc: 0.4906\n",
      "Epoch 91/100\n",
      "316/316 [==============================] - 0s 50us/step - loss: 1.0505 - acc: 0.4715 - val_loss: 1.0438 - val_acc: 0.4906\n",
      "Epoch 92/100\n",
      "316/316 [==============================] - 0s 78us/step - loss: 1.0504 - acc: 0.4715 - val_loss: 1.0438 - val_acc: 0.4906\n",
      "Epoch 93/100\n",
      "316/316 [==============================] - 0s 91us/step - loss: 1.0502 - acc: 0.4715 - val_loss: 1.0437 - val_acc: 0.4906\n",
      "Epoch 94/100\n",
      "316/316 [==============================] - 0s 105us/step - loss: 1.0502 - acc: 0.4715 - val_loss: 1.0436 - val_acc: 0.4906\n",
      "Epoch 95/100\n",
      "316/316 [==============================] - 0s 105us/step - loss: 1.0501 - acc: 0.4747 - val_loss: 1.0434 - val_acc: 0.4906\n",
      "Epoch 96/100\n",
      "316/316 [==============================] - 0s 116us/step - loss: 1.0500 - acc: 0.4747 - val_loss: 1.0434 - val_acc: 0.4906\n",
      "Epoch 97/100\n",
      "316/316 [==============================] - 0s 54us/step - loss: 1.0498 - acc: 0.4747 - val_loss: 1.0433 - val_acc: 0.4906\n",
      "Epoch 98/100\n",
      "316/316 [==============================] - 0s 76us/step - loss: 1.0498 - acc: 0.4715 - val_loss: 1.0434 - val_acc: 0.4906\n",
      "Epoch 99/100\n",
      "316/316 [==============================] - 0s 72us/step - loss: 1.0496 - acc: 0.4715 - val_loss: 1.0433 - val_acc: 0.4906\n",
      "Epoch 100/100\n",
      "316/316 [==============================] - 0s 82us/step - loss: 1.0495 - acc: 0.4747 - val_loss: 1.0431 - val_acc: 0.4906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2d462d30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trm = trained_model.predict(x_train.reshape(-1,70,30,1))\n",
    "X_test_trm = trained_model.predict(x_test.reshape(-1,70,30,1))\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "Classifier_input = Input((3,))\n",
    "Classifier_output = Dense(3, activation='softmax')(Classifier_input)\n",
    "Classifier_model = Model(Classifier_input, Classifier_output)\n",
    "\n",
    "\n",
    "Classifier_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "Classifier_model.fit(X_train_trm,y_train, validation_data=(X_test_trm,y_test),epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
